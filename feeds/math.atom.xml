<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Yet another site</title><link href="/" rel="alternate"></link><link href="/feeds/math.atom.xml" rel="self"></link><id>/</id><updated>2019-05-01T21:57:38+02:00</updated><entry><title>Chi square</title><link href="/chi-square.html" rel="alternate"></link><updated>2019-05-01T21:57:38+02:00</updated><author><name>Nekoo</name></author><id>tag:,2019-05-01:chi-square.html</id><summary type="html">&lt;p&gt;https://en.wikipedia.org/wiki/Chi-squared_test&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Goodness_of_fit&lt;/p&gt;
&lt;p&gt;https://robotics.stackexchange.com/questions/6447/role-of-chi2-in-slam-back-end-optimization&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\chi2\)&lt;/span&gt; is the squared Mahalanobis distance between predicted value and estimated value in SLAM.&lt;/p&gt;
&lt;p&gt;https://github.com/RainerKuemmerle/g2o/issues/185&lt;/p&gt;
&lt;p&gt;https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests#chi-square-goodness-of-fit-tests&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="TAGS"></category></entry><entry><title>Frame pose</title><link href="/frame-pose.html" rel="alternate"></link><updated>2019-04-01T22:22:58+02:00</updated><author><name>Nekoo</name></author><id>tag:,2019-04-01:frame-pose.html</id><summary type="html">&lt;div class="math"&gt;$$p_{W} = W_{A}T * p_{A}$$&lt;/div&gt;
&lt;div class="math"&gt;$$p_{A} = A_{W}T * p_{W} = (W_{A}T)^{-1} * p_{w}$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{A}T\)&lt;/span&gt;, frame a's pose in frame W, or read as "transform a point in A's frame
into W"&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(A_{W}T\)&lt;/span&gt; or &lt;span class="math"&gt;\((W_{W}T)^{-1}\)&lt;/span&gt;, inverse of frame a's pose in frame W, or read as "transform a point in W's frame
into A"&lt;/p&gt;
&lt;p&gt;http://library.isr.ist.utl.pt/docs/roswiki/tf(2f)Overview(2f)Transformations.html&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>Covariance matrix</title><link href="/covariance-matrix.html" rel="alternate"></link><updated>2019-03-17T14:29:50+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-03-17:covariance-matrix.html</id><summary type="html">&lt;h2&gt;Different covariance types for Gaussian Mixture Models&lt;/h2&gt;
&lt;p&gt;https://stats.stackexchange.com/questions/326671/different-covariance-types-for-gaussian-mixture-models&lt;/p&gt;
&lt;p&gt;http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/&lt;/p&gt;
&lt;p&gt;https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Factor Graph</title><link href="/factor-graph.html" rel="alternate"></link><updated>2019-03-02T19:09:52+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-03-02:factor-graph.html</id><summary type="html">&lt;h2&gt;High level problem definition and algorithm&lt;/h2&gt;
&lt;p&gt;Factor graph vs Pose graph&lt;/p&gt;
&lt;p&gt;elimination
Sliding window, remove unrelated pose and constraints
marginalization, keep the size of graph constant while maintain the information.&lt;/p&gt;
&lt;p&gt;Batch Estimation
Sliding window&lt;/p&gt;
&lt;h2&gt;Software package&lt;/h2&gt;
&lt;p&gt;g2o vs ceres solver vs GTSAM vs AprilSAM vs iSAM
&lt;em&gt; 性能，维护，扩展升级性，依赖其他库
&lt;/em&gt; Third party dependency:
&lt;em&gt; Sparse matrix, boost, eigen
&lt;/em&gt; open source/free software Licence
    * LGPL vs GPL vs MIT vs BSD etc.
| package | Licence |
---|---
 CSPARSE | LGPL v2.1+
 Suitesparse | LGPL v3+
 g2o | BSD
 ceres | BSD
 Eigen &amp;lt; 3.1.1 | LGPL3+
 Eigen 3.1.1 | MPL2
 iSAM | LGPL v2.1+&lt;/p&gt;
&lt;h2&gt;Hardware&lt;/h2&gt;
&lt;p&gt;performance requirements
&lt;em&gt; real-time
    * fast
    * predictability
&lt;/em&gt; no OS?
* cache?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SIMD, vectorization, compiler的局限, library&lt;/li&gt;
&lt;li&gt;multithreading&lt;/li&gt;
&lt;li&gt;multi-threaded BLAS&lt;/li&gt;
&lt;li&gt;优化分层，先上层c/c++优化，再硬件优化，汇编优化&lt;/li&gt;
&lt;li&gt;profiling, performance bottleneck&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;项目管理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;git submodule, project system structure
git code review and repository management
commit notification, achieve, summary generation
gerrit ?&lt;/li&gt;
&lt;li&gt;sandbox user branch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;review, validate and submit change
https://review.openstack.org/Documentation/intro-quick.html&lt;/p&gt;
&lt;p&gt;CI jenkins
&lt;em&gt; project release branch
&lt;/em&gt; project progress/ticket tracking
* project quality tracking
    * regression tracking
    * performance tracking
        * build time
        * runtime on given dataset&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>ICP</title><link href="/icp.html" rel="alternate"></link><updated>2019-02-07T17:54:55+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-02-07:icp.html</id><summary type="html">&lt;p&gt;Iterative Closest Point Algorithm主要由两部分组成&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data Association&lt;ul&gt;
&lt;li&gt;Closest-Point Matching&lt;br /&gt;
Kd-trees&lt;/li&gt;
&lt;li&gt;Normal Shooting&lt;br /&gt;
Slightly better convergence results than closest point for smooth structures,
worse for noisy or complex structures&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Error Metric:&lt;ul&gt;
&lt;li&gt;point to point&lt;br /&gt;
  SVD解法&lt;/li&gt;
&lt;li&gt;point to plane&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
Each iteratrion generally slower than the point-to-point versin, however, often
significantly better convergence rates.&lt;br /&gt;
solved using standard linear least-squares problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;基于k-d tree的ICP算法计算分析&lt;/h2&gt;
&lt;p&gt;使用k-d tree来加速整个过程的计算。总体计算花销分为两方面:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style="color:red"&gt; 从参考点云建立k-d树需要花销 &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style="color:red"&gt; k-d tree 查找的花销，其中主要是三个步骤，DFS查找，
ball-within-bounds test和backtracking. &lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style="color:red"&gt; 同时，因为使用了kd-tree数据结构，那么tree的平衡性，
会对整个查找复杂度有很大影响 &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在SLAM中，得到t时刻与t+1时刻的两幅环境的稀疏扫描点云，需要point
registreation来计算两个时刻间，机器人的位姿(pose)变换(rotation + translation)。
这当中包含一次k-d tree的简历，与许多次的ICP迭代，每次迭代，都需要对每一个query
point进行k-d tree的查找。&lt;/p&gt;
&lt;p&gt;有很多的优化方法，让r-d tree的建立，查找操作都更加快速。这些优化对于点云的拓扑结构
有一些假设。这些假设，在机器人应用场景下，不一定成立。大多时候，场景通过机器人的
传感器稀疏采样的。&lt;/p&gt;
&lt;h3&gt;cached k-d tree search for ICP algorithms&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;在论文中，作者有如下评论&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style="color:red"&gt;
However, these state of the art ICP variants all assume that the input data is
given as a mesh.  In many application scenarios a mesh is not available, e.g.,
3D data in robotics. Here, measurements contain in addition to Gaussian noise so
called salt-and-pepper noise. Furthermore, in robotics the scenes are often
sparsely sampled by the sensor. For these two reasons, simple meshing methods
based on the topology of the acquired points cannot be applied and roboticists
stick to using the raw point clouds. In this case the point-to-point metric,
cf. Eq. (1), and closest point search have to be used.
&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以传统的point-to-point error metric, k-d tree based closed point
search还是主流。目前我还没有阅读其他论文，所以无法评论作者观点是否正确。但是如果
真的这样，就只能在机器人场景下，，使用比较传统的方式。但是，r-d
tree的构建，查找还是可以利用硬件特性，例如GPU进行加速。&lt;/p&gt;
&lt;p&gt;在这篇论文中，作者改进了backtracking的思路，减少DFS搜索次数，同时使得backtracking
操作大致constant time。这是利用ICP的迭代算法特性，因为如果整个过程收敛，越到后面，
变换矩阵会越来越小，通过记录上一个iteration，查询点在r-d
tree中大致的位置，可以在当前iteration中大致估计，也会在差不多的位置。所以，不同于从
root开始重新查找一遍，这次从上一次的leaf node开始，反向查找。&lt;/p&gt;
&lt;p&gt;作者给出了性能比较，整体性能提升很明显。在这里，bucket size对性能也有影响。
通常对于k-d tree的讲解资料中，bucket size是1。在作者给出的性能对比中，cached
kd-tree在bucket size为1的时候提升最大。同时值得注意的是，bucket size
为1的时候，整体性能是最差的。所以为了寻求绝对性能的最大，还是要尝试不同的bucket
size。&lt;/p&gt;
&lt;p&gt;在文中，作者同时提到&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style="color:red"&gt;
The overall performance of the ICP algorithm depends
both on the search time and on the construction time
of the tree. However, the construction time of the trees
seems to be negligible.
&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在他的测量中，kd-tree的建立花销对整体性能影响不大，这也需要自己实际去测量。&lt;/p&gt;
&lt;p&gt;cached kd-tree需要一块内存在缓存上一迭代的结构，同时因为需要回溯的缘故，
每一个树节点需要保存一个父节点的指针。所以它会有额外的存储花销&lt;span class="math"&gt;\(O(N_d) +
O(N_m)\)&lt;/span&gt;, &lt;span class="math"&gt;\(N_m\)&lt;/span&gt;为reference data cloud point点数，&lt;span class="math"&gt;\(N_d\)&lt;/span&gt;为待匹配点云点数。&lt;/p&gt;
&lt;h3&gt;kd-tree construction optimization&lt;/h3&gt;
&lt;p&gt;普通情况下，需要寻找以当前划分坐标轴为衡量标准的所有点的中位数(median)，在naive
的算法里, median-finding algorithm复杂度为&lt;span class="math"&gt;\(O(n)\)&lt;/span&gt;，使用排序的方法的话，
可以优化到&lt;span class="math"&gt;\(nO(log\;n)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;具体可参考&lt;a href="https://en.wikipedia.org/wiki/K-d_tree#Complexity"&gt;wiki, ICP Complexity&lt;/a&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;http://ais.informatik.uni-freiburg.de/teaching/ss10/robotics/slides/17-icp.pdf&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="/pdfs/Cachedk-d_tree_search_for_ICP_algorithms.pdf"&gt;Cached k-d tree search for ICP algorithms&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;https://www.comp.nus.edu.sg/~lowkl/publications/lowk_point-to-plane_icp_techrep.pdf&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>Robust regression</title><link href="/robust-regression.html" rel="alternate"></link><updated>2019-02-01T01:06:33+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-02-01:robust-regression.html</id><summary type="html">&lt;p&gt;最小二乘法对于outlier比较敏感&lt;/p&gt;
&lt;p&gt;对于高斯噪声，最大似然等同于最小二乘
https://www.jianshu.com/p/d0ea25071c57&lt;/p&gt;
&lt;p&gt;So for least squares to have a useful statistical interpretation, the Wi should 
be chosen to approximate the inverse measurement covariance of z i.
Even for non-Gaussian noise with this mean and covariance, the Gauss-Markov
theorem [37, 11] states that if the models zi (x) are linear, least squares
gives the Best Linear Unbiased Estimator (BLUE), where ‘best’ means minimum
variance.&lt;/p&gt;
&lt;p&gt;Least square with covariance as weight matrix??&lt;/p&gt;
&lt;h2&gt;robust regression&lt;/h2&gt;
&lt;p&gt;loss function&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>卡尔曼滤波，最大似然，最小二乘</title><link href="/qia-er-man-lu-bo-zui-da-si-ran-zui-xiao-er-cheng.html" rel="alternate"></link><updated>2019-01-28T23:33:07+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-28:qia-er-man-lu-bo-zui-da-si-ran-zui-xiao-er-cheng.html</id><summary type="html">&lt;p&gt;最小二乘估计：不考虑数据的统计特性，如期望，方差等，直接用最小二乘法得到最优估计。
最小二乘估计只保证测量值与估计值的平方和最小，不保证估计误差的方差最小。最小二乘估计不需要随机变量V的任何统计信息。&lt;/p&gt;
&lt;p&gt;测量误差（测量）服从高斯分布的情况下， 最小二乘法等价于极大似然估计。(负对数)&lt;/p&gt;
&lt;p&gt;卡尔曼误差高斯分布后验概率推导
http://www.cnblogs.com/ycwang16/p/5999034.html&lt;/p&gt;
&lt;p&gt;卡尔曼最小均方差推导
卡尔曼滤波就是递推最小二乘法的一种特殊情况，卡尔曼滤波也是去通过最小化方差来求得最优的估计值。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/ethan_guo/article/details/80568254"&gt;最大似然估计,最小二乘估计,卡尔曼滤波,三者的相互关系&lt;/a&gt;
&lt;a href="https://blog.csdn.net/qinruiyan/article/details/50793114"&gt;SLAM学习笔记2：Kalman Filter(卡尔曼滤波) 与Least Square(最小二乘法) 的比较&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Hessian</title><link href="/hessian.html" rel="alternate"></link><updated>2019-01-27T15:13:19+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-27:hessian.html</id><summary type="html">&lt;h1&gt;Hessian&lt;/h1&gt;
&lt;h1&gt;Hessian positive semidefinite&lt;/h1&gt;
&lt;h1&gt;Hessian ill conditioned, eigen value&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;convergence rate&lt;/li&gt;
&lt;li&gt;numeric statibility&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;https://www.zhihu.com/question/56977045
这里有一个知乎问答 &lt;a href="https://www.zhihu.com/question/24623031"&gt;Hessian 矩阵的特征值有什么含义&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Hessian approximation&lt;/h1&gt;
&lt;p&gt;高斯牛顿&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Quadratic forms</title><link href="/quadratic-forms.html" rel="alternate"></link><updated>2019-01-27T13:47:50+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-27:quadratic-forms.html</id><summary type="html">&lt;p&gt;最近在凸优化&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;一书中，steepest descent相关章节看到quadratic norm(9.4)，
gradient descent与steepest descent的却别就在于不同范数(norm)的选取&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$\Delta_{nsd}=argmin_v(\nabla f(x)^Tv\mid \|v\|&amp;lt;=1)$$&lt;/div&gt;
i.e.  , as the direction in the unit ball of ‖·‖ that extends farthest in the
direction &lt;span class="math"&gt;\(-\nabla f(x)\)&lt;/span&gt;&lt;br /&gt;
Steepest descent for quadratic norm&lt;br /&gt;
We consider the quadratic norm
&lt;div class="math"&gt;$$\Vert z \Vert_{P} = (z^T P z)^{1/2} = \Vert P^{1/2}z\Vert_{2}$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不太清楚它的形式，找到如下资料&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/quadratic_forms_1.png" alt="quadratic forms" title="" style="max-width:80%; max-height:80%"/&gt;
&lt;img src="images/quadratic_forms_2.png" alt="quadratic forms" title="" style="max-width:80%; max-height:80%"/&gt;
&lt;img src="images/quadratic_forms_3.png" alt="quadratic forms" title="" style="max-width:80%; max-height:80%"/&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;http://stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;https://see.stanford.edu/materials/lsoeldsee263/15-symm.pdf&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/Timingspace/article/details/5096356://blog.csdn.net/Timingspace/article/details/50963564"&gt;梯度下降法和最速下降法的细微差别&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>Geometric Operations</title><link href="/geometric-operations.html" rel="alternate"></link><updated>2019-01-26T17:49:37+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-26:geometric-operations.html</id><summary type="html">&lt;h1&gt;Translation&lt;/h1&gt;
&lt;h1&gt;Rotation&lt;/h1&gt;
&lt;h1&gt;Affine transformation&lt;/h1&gt;
&lt;p&gt;线性变换+平移, 6 DOF&lt;/p&gt;
&lt;h1&gt;Rigid transformation (Euclidean transform, SE(3))&lt;/h1&gt;
&lt;p&gt;Any Euclidean transformation is an affine transformation. But not the other way
around, for example, Reflections, Shear transformation.&lt;/p&gt;
&lt;p&gt;6 Degrees Of Freedom
Transformation matrix = rotation + translation.&lt;/p&gt;
&lt;h1&gt;仿射函数与线性函数区别&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;A linear function fixes the origin, whereas an affine function need not do so.
An affine function is the composition of a linear function with a translation,
so while the linear part fixes the origin, the translation can map it
somewhere else.&lt;/p&gt;
&lt;p&gt;Linear functions between vector spaces preserve the vector space structure
(so in particular they must fix the origin). While affine functions don't
preserve the origin, they do preserve some of the other geometry of the
space, such as the collection of straight lines.&lt;/p&gt;
&lt;p&gt;If you choose bases for vector spaces V
and W of dimensions m and n respectively, and consider functions f:V→W, then
f is linear if f(v)=Av for some n×m matrix A and f is affine if f(v)=Av+b
for some matrix A and vector b, where coordinate representations are used
with respect to the bases chosen.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="Math"></category></entry><entry><title>Leventberg-Marquardt Algorithm</title><link href="/leventberg-marquardt-algorithm.html" rel="alternate"></link><updated>2019-01-25T12:31:55+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-25:leventberg-marquardt-algorithm.html</id><summary type="html">&lt;p&gt;最近想弄懂LM算法，看了一些介绍。 虽然不重要，但是对于它到底是使用line search和trust region算法感到困惑。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Levenberg (1944) and later Marquardt (1963) suggested to use a
damped Gauss-Newton method, cf Section 2.4.  The step &lt;span class="math"&gt;\(h_{lm}\)&lt;/span&gt; is defined by the
following modification to (3.9),&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;
&lt;div class="math"&gt;$$(J^TJ + \lambda I)h_{lm} = -g \;with\; g = J^Tf \;and\; \lambda \geqslant 0$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，最初LM算法的提出是以一种damped Gauss-Newton的方式， 而没有使用trust
region的概念。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The above algorithm has the disadv antage that if the value of &lt;span class="math"&gt;\(lambda\)&lt;/span&gt; is large,
the calculated Hessian matrix is not used at all.  We can deri ve some advantage
out of the second deri vative even in such cases by scaling each component of
the gradient according to the curv ature.  This should result in lar ger movement
along the directions where the gradient is smaller so that the classic
“error valley” problem does not occur any more.  This crucial insight was
provided by Marquardt.  He replaced the identity matrix in (7) with the diagonal
of the Hessian resulting in the Levenberg-Marquardt update rule.  &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;
&lt;div class="math"&gt;$$(H + \lambda diag[H])h_{lm} = -g \;with\; g = J^Tf \;and\; \lambda \geqslant 0$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;同时它是一个基于经验(heuristic)的算法, 是从gradient
descent和Gauss-Newton算法总观察改进而来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gradient descent一阶算法简单, 快速，但是有各种收敛问题,
  确定的方向，但是更新步长没有根据梯度变化而调整。&lt;/li&gt;
&lt;li&gt;同时牛顿法为二阶，使用second derivative。从几何上说，牛顿法就是用一个二次曲面
去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，
通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的
最优下降路径。由于精确的Hessian矩阵求解计算量大，可以使用: &lt;span class="math"&gt;\(H \approx J^TJ\)&lt;/span&gt;
近似Hessian矩阵，这就是Gauss-Newton算法。但是高斯牛顿算法有一个问题在于，对于初始点比较敏感。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结合二者的优缺点，提出的LM算法，根据&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;的值相对大小，
LM算法体现出梯度下降或者高斯牛顿算法的特征，同时根据误差计算，调整damping factor
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;大小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;但是，这个经验公式在实际问题中，却非常有效。因为它跟基于trust region的高斯牛顿
法得到的迭代公式完全一样。所以有了优化的理论根据！&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;LM算法基本流程&lt;/h1&gt;
&lt;h1&gt;基于信赖域的Gauss-Newton法推导&lt;/h1&gt;
&lt;h2&gt;non-linear least squares problem&lt;/h2&gt;
&lt;p&gt;总是把对不同函数的求导弄混，这里重写写一遍，加深理解。
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\xv}{\mathbf{x}}
\xv^{*} = argmin_{\xv}\{F(\xv)\}$$&lt;/div&gt;
&lt;p&gt;
where
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\xv}{\mathbf{x}} \newcommand{\fv}{\mathbf{f}}
F(\xv)=\frac{1}{2}\sum_{i=1}^{m}(f_{i}(x))^2\;=\;
\frac{1}{2}\Vert \fv(\xv) \Vert^2\;=\;
\frac{1}{2}\fv(\xv)^{T}\fv(\xv)$$&lt;/div&gt;
&lt;p&gt;根据泰勒展开：
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\xv}{\mathbf{x}} \newcommand{\hv}{\mathbf{h}}
\newcommand{\fv}{\mathbf{f}}
\fv(\xv+\hv) \;=\; \fv(\xv) + \mathbf{J}(\xv)\hv + O(\Vert \hv \Vert )^2 $$&lt;/div&gt;
&lt;p&gt;
这里&lt;span class="math"&gt;\(\mathbf{J}\)&lt;/span&gt;为&lt;span class="math"&gt;\(\mathbf{f}(\mathbf{x})\)&lt;/span&gt;的Jacobian矩阵(不是&lt;span class="math"&gt;\(F(\mathbf{x})\)&lt;/span&gt;)。
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\xv}{\mathbf{x}} \newcommand{\fv}{\mathbf{f}}
\def\pdiff#1#2{\frac{\partial #1}{\partial #2}}
\pdiff{F}{x_j}(\xv) \;=\; \sum_{i=1}^{m}f_{i}(\xv)\pdiff{f_i}{x_j}(\xv) \\
F^{'}(\xv) \;=\; J(\xv)^{T}\fv(\xv)$$&lt;/div&gt;
&lt;p&gt;对&lt;span class="math"&gt;\(\mathbf{f}\)&lt;/span&gt;泰勒展开
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\xv}{\mathbf{x}} \newcommand{\hv}{\mathbf{h}}
\newcommand{\fv}{\mathbf{f}} \newcommand{\jv}{\mathbf{J}}
\fv(\xv + \hv) \;=\; \ell(\hv) \approx \fv(\xv) + \mathbf{J}(\xv)\hv \\
F(\xv+\hv) \;=\; L(\hv) \;=\; \frac{1}{2}\ell(\hv)^{T}\ell(\hv) \\
=\; \frac{1}{2}{\fv}^T\fv + {\hv}^T\jv\fv + \frac{1}{2}{\hv}^T{\jv}^T\hv \\
=\; F(\xv) + {\hv}^T{\jv}^T\fv + \frac{1}{2}\hv^T{\jv}^T\jv\hv
=\; F(\xv) + {\fv}^T{\jv}\hv + \frac{1}{2}\hv^T{\jv}^T\jv\hv
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(L\)&lt;/span&gt;的gradient与Hessian为:
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\xv}{\mathbf{x}} \newcommand{\hv}{\mathbf{h}}
\newcommand{\fv}{\mathbf{f}} \newcommand{\jv}{\mathbf{J}}
L^{'} \;=\; \jv^T\fv + \jv^T\jv\hv, ;\ L^{''} \;=\; \jv^T\jv$$&lt;/div&gt;
&lt;p&gt;
对于高斯牛顿法，求&lt;span class="math"&gt;\(\mathbf{h}_{gn}\)&lt;/span&gt;使得&lt;span class="math"&gt;\(L(\mathbf{h})\)&lt;/span&gt;最小：
&lt;/p&gt;
&lt;div class="math"&gt;$$\newcommand{\hv}{\mathbf{h}}
\newcommand{\jv}{\mathbf{J}}
\newcommand{\fv}{\mathbf{f}}
\hv_{gn} \;=\; argmin_{\hv}{L(\hv)} \\
L^{'}(\hv) \;=\; 0 \\
({\jv}^T\jv)\hv_{gn} \;=\; -{\jv}^T\fv
$$&lt;/div&gt;
&lt;h2&gt;基于信赖域的Gauss-Newton法推导&lt;/h2&gt;
&lt;div class="math"&gt;$$L({\Delta x_k}) = F(x_k + \Delta x_k) \approx f(x_k) + J_{x_k}{\Delta x_k} + \frac{1}{2}{\Delta x_k}H_{x_k}{\Delta x_k}$$&lt;/div&gt;
&lt;p&gt;
求解子问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$argmin_{\Delta x_k} L({\Delta x_k}), \; s.t. \Vert D\Delta x_k \Vert^2 \leq \mu \; (1)$$&lt;/div&gt;
&lt;p&gt;
信赖域法的核心问题是一个不等式约束问题，可以通过Lagrange将它转换成一个无约束问题。
&lt;/p&gt;
&lt;div class="math"&gt;$$argmin_{\Delta x_k} L({\Delta x_k}) \;+\; \frac{\lambda}{2}\Vert D\Delta x_k \Vert^2\; (2)$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;为Lagrange乘子。展开得：
&lt;/p&gt;
&lt;div class="math"&gt;$$(H + \lambda D^TD)\Delta x_k \;=\; -g \;with\; g = J^Tf$$&lt;/div&gt;
&lt;p&gt;
当去D为单位矩阵I时，得到的更新求解公式与之前的经验公式一样。
&lt;a href="http://pages.mtu.edu/~msgocken/ma5630spring2003/lectures/tr/tr/node2.html"&gt;The trust region subproblem&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;注意点&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(lambda\)&lt;/span&gt;取值要使得&lt;span class="math"&gt;\((\mathbf{J}^T\mathbf{J} + \lambda I)\)&lt;/span&gt;为正定矩阵positive
definit, 这样&lt;span class="math"&gt;\(\Delta x_k\)&lt;/span&gt;才是&lt;span class="math"&gt;\(L\)&lt;/span&gt;的minimizer.&lt;/li&gt;
&lt;li&gt;关于D的选取，有的选取为单位矩阵，有的选取为Hessian矩阵对角元素&lt;/li&gt;
&lt;li&gt;$(H + \lambda diag H) 的condition number可能会很大&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;。或者因为精度问题，导致H对
角线上元素为0. 对这个表达式进行矩阵分解求解方程的时候，需要注意算法的stability.
(可能考虑使用QR decomposition&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里有个一LM的算法实现，注意上面提到的两点
https://gist.github.com/lirenlin/ce6159058348efa1a9f7c8d621c1e768&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="orbit.dtu.dk/files/2721358/imm3215.pdf"&gt;methods for non-linear least squares problems&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="http://ananth.in/docs/lmtut.pdf"&gt;The Levenberg-Marquardt Algorithm &lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;https://sites.math.washington.edu/~reu/papers/2009/mark/REU%20Final%20Paper.pdf&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;https://mathoverflow.net/questions/106277/levenberg-marquadt-near-the-minima-for-non-zero-residual-problems://mathoverflow.net/questions/106277/levenberg-marquadt-near-the-minima-for-non-zero-residual-problems/ &amp;#160;&lt;a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>Numerical stability</title><link href="/numerical-stability.html" rel="alternate"></link><updated>2019-01-23T14:27:51+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-23:numerical-stability.html</id><summary type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Numerical_stability"&gt;Wiki: numerical stability&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"&gt;What Every Computer Scientist Should Know About Floating-Point Arithmetic&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>cholesky decomposition</title><link href="/cholesky-decomposition.html" rel="alternate"></link><updated>2019-01-22T16:48:47+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-22:cholesky-decomposition.html</id><summary type="html">&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Linear least squares, the solution is to solve this system of linear
equations: &lt;span class="math"&gt;\(A^TAx = TX = A^Tb\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Non-linear least squares problem via iterative method&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gauss-Newton Method, solve &lt;span class="math"&gt;\((J_x^T J_x)\Delta = T\Delta = -J_x^Tf(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Levenberg-Marquardt Algorithm, solve &lt;span class="math"&gt;\((J_x^T J_x + \lambda D)\Delta = T\delta = -J_x^Tf(x)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面所有的T都是symmetric矩阵，可以使用Cholesky decomposition来解决。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少计算量, 即使有额外的中间步骤需要计算&lt;/li&gt;
&lt;li&gt;the amount of fill-in which occours when computing a matrix decomposition of a sparse matrix&lt;/li&gt;
&lt;li&gt;stable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cholesky decomposition &lt;span class="math"&gt;\(A = LL^T\)&lt;/span&gt;. This is stable even without
pivoting, and hence extremely simple to implement.
It is the standard decomposition method for almost all unconstrained optimization
problems including bundle adjustment, as the Hessian is positive definite near
a non-degenerate cost minimum (and in the Gauss-Newton approximation, almost
everywhere else, too). If A is symmetric but only positive semidefinite,
diagonally pivoted Cholesky decomposition can be used.&lt;/p&gt;
&lt;p&gt;Another notable method is that based on QR decomposition, which is up to a
factor of two slower than the normal equations, but much less sensitive to
ill-conditioning in J.&lt;/p&gt;
&lt;p&gt;The triangular factor L and a solution to a corresponding linear system may not
be accurate enough because of machine arithmetic. In order to refine the
solution, a number of iterative methods (for example, the conjugate
 gradient method) can be employed using the &lt;span class="math"&gt;\(LL^T\)&lt;/span&gt; decomposition as a
preconditioner.  The memory saving is the main reason to use an incomplete or
inaccurate decomposition as a preconditioner.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;https://algowiki-project.org/en/Cholesky_method&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>ill-conditioned Matrices</title><link href="/ill-conditioned-matrices.html" rel="alternate"></link><updated>2019-01-21T14:50:04+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-21:ill-conditioned-matrices.html</id><summary type="html">&lt;h1&gt;What is an ill-conditioned matrix?&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;p&gt;A matrix can represent a mapping from one space to another space. The
conditioning number of a matrix gives us the ratio of how crazy this mapping
can be. The conditioning number is the ratio of the largest singular value to
the smallest singular value.&lt;/p&gt;
&lt;p&gt;A matrix is ill-conditioned if the conditioning number is very high. What this
means is that calculations using this matrix are prone to introduce numerical
errors that can overwhelm your calculation. Computers cannot hold an infinite
amount of information. Numbers in a floating point representation can only hold
so much precision.&lt;/p&gt;
&lt;p&gt;For example if 𝐴 is ill-conditioned and I try to solve 𝐴𝑥=𝑏
using Gauss-Jordan elimination I will quickly pick up numerical errors, and
then those numerical errors are used on the next step which creates even larger
errors, which are used on the next step and so on.&lt;/p&gt;
&lt;p&gt;So what do you do when you have an ill-conditioned matrix? You reformulate
your problem by doing something called preconditioning. This allows you to work
in a space where your transformations aren’t as badly behaved and errors can be
kept under control.&lt;/p&gt;
&lt;h1&gt;Poorly conditioned Hessian matrix&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;p&gt;Condition number of a matrix is the ratio of the largest singular value to the
smallest singular value. A matrix is ill-conditioned if the condition number is
very high, usually indicating that (i) the lowest singular value is orders of
magnitude smaller than the highest one, and (ii) columns/rows of the matrix are
heavily correlated with each other leading to redundancies and a matrix that is
pretending to be of a higher rank than it truly is.&lt;/p&gt;
&lt;p&gt;Hessian encodes the second derivatives of a function with respect to all pairs
of variables. So, if there are n inputs to a function, the gradient is 𝑛
-dimensional and the Hessian is 𝑛x𝑛-dimensional. In machine learning, the inputs
are usually the features and the function is usually a loss function we are
trying to minimize. When the Hessian is ill-conditioned, it means the basins of
the loss function have contours that are very long “ellipsoids” rather than being close to “circular”.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/ic-1.gif" alt="01" title="01" style="max-width:80%;max-height:80%"/&gt;&lt;/p&gt;
&lt;p&gt;This causes problems for first-order optimization methods like gradient descent
(𝑤=𝑤−𝜂∇𝑤) which need to follow a very zigzag path to the minimum. The first
figure below shows the gradient directions at various points on a straight line
through the parameter space. Notice how the directions away from the center are
nearly orthogonal to the useful direction of descent. The second figure shows
the zigzag path gradient descent has to take if the Hessian is ill-conditioned,
the function contours are stretched out, and the gradients often point to directions
that might not be the best way to descend to the minimum.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/ic-2.png" alt="02" title="02" style="max-width:80%;max-height:80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/ic-3.png" alt="03" title="03" style="max-width:80%;max-height:80%"/&gt;&lt;/p&gt;
&lt;p&gt;You would think that second-order optimization would solve this problem. It sort
of does if the Hessian is not too ill-conditioned. However, a poorly conditioned
Hessian is problematic because a typical second-order optimization requires
inversion of a Hessian: 𝑤=𝑤−𝜂𝐻−1∇𝑤 (First-order optimization methods therefore
assume 𝐻=𝐼, the identity matrix i.e. the problem is ideally conditioned for
gradient descent-like methods.) Inverse of 𝐻with SVD decomposition 𝐻=𝑈Σ𝑉𝑇 is
given as 𝐻−1=𝑈Σ−1𝑉𝑇. Here Σ is a diagonal matrix with singular values on the
diagonal. Note, if the Hessian is ill-conditioned, the inverse can be numerically
unstable since the smallest singular values blow up on inversion. A singular
value of 10−6 appears as 106 in Σ−1. Thus, the more ill-conditioned the Hessian
is, the more numerically unstable its inverse. Any noise in computing the Hessian
such as that introduced by using stochastic versions of descent updates or using
minibatches amplifies tremendously when the Hessian is inverted. Methods likes
L-BFGS get around this by maintaining a low-rank approximation of the (inverse)
Hessian which is better suited for ill-conditioned problems as well as saves
computation and space required to implement second-order optimization.&lt;/p&gt;
&lt;p&gt;这里有一个知乎问答 &lt;a href="https://www.zhihu.com/question/24623031"&gt;Hessian 矩阵的特征值有什么含义&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个答案给出了一些有背景的回答：https://www.zhihu.com/question/24623031/answer/118562562
steepest descent里面，可以使用Hessian范数，来求更新步长方向，也就是牛顿法。Hessian特征值差异大，所定义的空间椭圆空间长轴短轴比例越失调，相反，如果是一个球星空间，则各个方向一致, 这个时候收敛更快。&lt;/p&gt;
&lt;h1&gt;&lt;a href="https://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/04LinearAlgebra/illconditioned/"&gt;uwaterloo: Numerical Analysis: ill conditioned matrices&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Numerical_stability"&gt;Wiki: numerical stability&lt;/a&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;https://www.quora.com/What-is-an-ill-conditioned-matrix&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;https://www.quora.com/What-does-it-mean-to-have-a-poorly-conditioned-Hessian-matrix&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="Math"></category></entry><entry><title>一阶二阶优化方法</title><link href="/yi-jie-er-jie-you-hua-fang-fa.html" rel="alternate"></link><updated>2019-01-21T11:15:42+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-21:yi-jie-er-jie-you-hua-fang-fa.html</id><summary type="html">&lt;p&gt;1, 计算量, Hessian矩阵
2, 收敛迭代次数, 更少的迭代次数，但是每次计算量更大
3, 问题数值精度
4, 数值稳定性, 算法更复杂&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>ill-conditioned problem</title><link href="/ill-conditioned-problem.html" rel="alternate"></link><updated>2019-01-18T15:09:54+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-18:ill-conditioned-problem.html</id><summary type="html">&lt;p&gt;在SLAM，BA中，经常提到ill conditioned problems，需要转换矩阵形式，提供reliable,
robust and efficient的系统结果。而这些在系统行为又在相关应用中尤为重要，
所以需要细心的学习。&lt;/p&gt;
&lt;p&gt;文章中建议到:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;unless you are familiar with these issues, it is advisable to use profession-
ally designed methods
但是至少我们要知道这个事情的存在，与相关考虑的必要性。&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="Math"></category></entry><entry><title>约束优化</title><link href="/yue-shu-you-hua.html" rel="alternate"></link><updated>2019-01-16T12:19:15+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-16:yue-shu-you-hua.html</id><summary type="html">&lt;p&gt;&lt;a href="https://blog.csdn.net/touristman5/article/details/57418552"&gt;等式约束，不等式约束问题&lt;/a&gt;
&lt;a href="https://en.wikipedia.org/wiki/Constrained_optimization"&gt;wiki: constrained optimization&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>范数Norm</title><link href="/fan-shu-norm.html" rel="alternate"></link><updated>2019-01-15T15:47:30+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-15:fan-shu-norm.html</id><summary type="html">&lt;h1&gt;向量范数(norm)&lt;/h1&gt;
&lt;p&gt;From wiki:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In linear algebra, functional analysis, and related areas of mathematics,
a norm is a function that assigns a strictly positive length or size to
each vector in a vector space—except for the zero vector, which is
assigned a length of zero.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;矩阵范数&lt;/h1&gt;
&lt;p&gt;From wiki:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In mathematics, a matrix norm is a vector norm in a vector space whose elements
(vectors) are matrices (of given dimensions).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以从函数、几何与矩阵的角度去理解范数。&lt;/p&gt;
&lt;p&gt;我们都知道，函数与几何图形往往是有对应关系的，这个很好想象，特别是在三维以下的空间内，函数是几何图像的数学概括，而几何图像是函数的高度形象化，比如一个函数对应几何空间上若干点组成的图形。
但当函数与几何超出三维空间时，就难以获得较好的想象，于是就有了映射的概念，映射表达的就是一个集合通过某种关系转为另外一个集合。通常数学书是先说映射，然后再讨论函数，这是因为函数是映射的一个特例。
为了更好的在数学上表达这种映射关系，（这里特指线性关系）于是就引进了矩阵。这里的矩阵就是表征上述空间映射的线性关系。而通过向量来表示上述映射中所说的这个集合，而我们通常所说的基，就是这个集合的最一般关系。于是，我们可以这样理解，一个集合（向量），通过一种映射关系（矩阵），得到另外一个集合（另外一个向量）。
那么向量的范数表示这个原有集合的大小。
矩阵的范数表示这个变化过程的大小的一个度量。
简单说：0范数表示向量中非零元素的个数（即为其稀疏度）。1范数表示为，绝对值之和。而2范数则指模。&lt;/p&gt;
&lt;p&gt;具体向量范数与矩阵范数参见
&lt;a href="https://www.zhihu.com/question/20473040"&gt;知乎：0 范数、1 范数、2 范数有什么区别？&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://mathworld.wolfram.com/VectorNorm.html"&gt;mathworld: Vector Norm&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Pseudo Inverse</title><link href="/pseudo-inverse.html" rel="alternate"></link><updated>2019-01-11T18:03:09+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-11:pseudo-inverse.html</id><summary type="html">&lt;p&gt;In mathematics, and in particular linear algebra, a pseudoinverse A+ of a matrix
A is a generalization of the inverse matrix. The most widely known type of
matrix pseudoinverse is the Moore–Penrose inverse, which was independently
described by E. H. Moore in 1920, Arne Bjerhammar in 1951, and Roger Penrose in
1955. Earlier, Erik Ivar Fredholm had introduced the concept of a pseudoinverse
of integral operators in 1903. When referring to a matrix, the term
pseudoinverse, without further specification, is often used to indicate the
Moore–Penrose inverse. The term generalized inverse is sometimes used as a
synonym for pseudoinverse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A common use of the pseudoinverse is to compute a 'best fit' (least squares)
solution to a system of linear equations that lacks a unique solution
(see below under § Applications). Another use is to find the minimum
(Euclidean) norm solution to a system of linear equations with multiple
solutions. The pseudoinverse facilitates the statement and proof of results in
linear algebra.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The pseudoinverse is defined and unique for all matrices whose entries are real
or complex numbers. It can be computed using the singular value decomposition. &lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>线性变换的理解</title><link href="/xian-xing-bian-huan-de-li-jie.html" rel="alternate"></link><updated>2019-01-10T23:13:22+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-10:xian-xing-bian-huan-de-li-jie.html</id><summary type="html">&lt;p&gt;给定一个向量x
[数值] = [坐标] x [坐标系统基]&lt;/p&gt;
&lt;p&gt;linear transformation可以理解成坐标系统的改变.
例如将x变换到以[v1, v2, ... vn]为basis的系统下，&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
X &amp;amp;= c_{1}v_{1} + c_{2}v_{2} + ... c_{n}v_{n} \\
X &amp;amp;= [v_{1} v_{2} ... v_{n}] [c_{1} c_{2} ...]^{T} \\
X &amp;amp;= WC  (1)\\
C &amp;amp;= W^{-1}X (2)
\end{aligned}&lt;/div&gt;
&lt;p&gt;(2) 为变换，C为X变换后，在新的坐标系下的坐标. &lt;span class="math"&gt;\(W^{-1]\)&lt;/span&gt;为变换矩阵。
(1) 为逆变换，在给定C后，恢复在原始坐标系下的数据&lt;/p&gt;
&lt;p&gt;所以对于一个好的变换矩阵，需要有下面两个特征&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;快速的矩阵乘法与求逆，这样才可以快速进行运算。例如使用傅里叶变化矩阵的快速傅里叶变换与Wavelet变换矩阵&lt;/li&gt;
&lt;li&gt;好的压缩特性，尽量少的坐标轴包含尽量多的信息。这样可以忽略不重要的信息，进行降维，实现压缩&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=vGkn-3NFGck&amp;amp;t=1709s"&gt;Lec 31 | MIT 18.06 Linear Algebra, Spring 2005&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>SVD</title><link href="/svd.html" rel="alternate"></link><updated>2019-01-10T16:21:16+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-10:svd.html</id><summary type="html">&lt;h1&gt;特征值分解&lt;/h1&gt;
&lt;p&gt;要求: 方阵
线性代数中，特征分解（Eigendecomposition），又称谱分解（Spectral decomposition）
是将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法。
需要注意只有对可对角化矩阵才可以施以特征分解.&lt;/p&gt;
&lt;p&gt;令 A 是一个 N×N 的方阵，且有 N 个线性无关的特征向量 &lt;span class="math"&gt;\(q_{i}\,\,(i=1,\dots ,N)\)&lt;/span&gt;。
这样， A 可以被分解为
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbf {A} =\mathbf {Q} \mathbf {\Lambda } \mathbf {Q} ^{-1}$$&lt;/div&gt;
&lt;p&gt;
其中 Q 是N×N方阵，且其第 i列为 A 的特征向量 &lt;span class="math"&gt;\(q_{i}\)&lt;/span&gt;。 Λ 是对角矩阵，
其对角线上的元素为对应的特征值，也即 &lt;span class="math"&gt;\(\Lambda_{ii}=\lambda_{i}\)&lt;/span&gt;。这里需要注意
只有可对角化矩阵才可以作特征分解。 比如
&lt;/p&gt;
&lt;div class="math"&gt;$$\begin{bmatrix}1&amp;amp;1\\0&amp;amp;1\\\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
不能被对角化，也就不能特征分解。&lt;/p&gt;
&lt;p&gt;一般来说，特征向量 &lt;span class="math"&gt;\( q_{i}\,\,(i=1,\dots ,N)\)&lt;/span&gt; 一般被单位化（但这不是必须的）。
未被单位化的特征向量组 &lt;span class="math"&gt;\( v_{i}\,\,(i=1,\dots ,N)\)&lt;/span&gt;, 也可以作为 Q 的列向量。
这一事实可以这样理解： Q 中向量的长度都被&lt;span class="math"&gt;\(Q^{−1}\)&lt;/span&gt;抵消了。&lt;/p&gt;
&lt;h1&gt;谱分解&lt;/h1&gt;
&lt;p&gt;要求: 正规矩阵&lt;/p&gt;
&lt;h1&gt;奇异值分解Singular Value Decomposition&lt;/h1&gt;
&lt;h2&gt;定义&lt;/h2&gt;
&lt;p&gt;奇异值分解是一个能适用于任意的矩阵的一种分解的方法
SVD 分解说的是：假设 M 是一个 m×n 的矩阵，其中的元素全部属于数域 𝕂
（实数域 ℝ 或复数域 ℂ）。那么，存在 m×m 的酉矩阵 U 和 n×n 的酉矩阵 V
使得
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbf M = \mathbf U\mathbf\Sigma\mathbf V^{\mathsf H}$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; 是 m×n 的非负实数对角矩阵；并且&lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt; 对角线上的元素
&lt;span class="math"&gt;\(\Sigma_{i,i}\)&lt;/span&gt; 是 M 的奇异值。
一般来说，我们偏好将这些奇异值按从大到小的顺序排列，这样一来 Σ 就由 M 唯一确定了。
另一方面，因为 U 和 V 都是酉矩阵，所以 U 和 V 的列向量分别张成 𝕂m 和 𝕂n 的一组标
准正交基。我们将 U 的列向量记作 u⃗ i,1⩽i⩽m；将 V 的列向量记作 v⃗ j,1⩽j⩽n；同时，
将 Σ 对角线上的第 i 个元素记作 σk,1⩽k⩽min(m,n)。那么，SVD 分解实际可以将矩阵 M
写作一个求和形式
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbf M = \sum_{i = 1}^{\min(m, n)}\Sigma_i\vec u_i\vec v_i^{\mathsf T}$$&lt;/div&gt;
&lt;h2&gt;SVD计算&lt;/h2&gt;
&lt;h2&gt;SVD分解几何解释&lt;/h2&gt;
&lt;div class="math"&gt;$$\mathbf M = \mathbf U\mathbf\Sigma\mathbf V^{\mathsf H}$$&lt;/div&gt;
&lt;p&gt;
旋转, 缩放, 旋转&lt;/p&gt;
&lt;p&gt;U 和 V 都是旋转矩阵 (标准正交基)&lt;/p&gt;
&lt;h2&gt;SVD应用&lt;/h2&gt;
&lt;p&gt;SVD分解至少有两方面作用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分析了解原矩阵的主要特征和携带的信息（取若干最大的奇异值），这引出了主成分分析（PCA）；&lt;/li&gt;
&lt;li&gt;丢弃忽略原矩阵的次要特征和携带的次要信息（丢弃若干较小的奇异值），这引出了信息有损压缩、矩阵低秩近似等话题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;奇异向量的现实意义&lt;/h2&gt;
&lt;p&gt;SVD也可以得到协方差矩阵XTX最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵XTX，也能求出我们的右奇异矩阵V&lt;/p&gt;
&lt;h1&gt;参考整理来源&lt;/h1&gt;
&lt;p&gt;https://www.zhihu.com/question/22237507&lt;br /&gt;
https://liam.page/2017/11/22/SVD-for-Human-Beings/&lt;br /&gt;
https://www.cnblogs.com/pinard/p/6251584.html  &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>"美"的矩阵</title><link href="/mei-de-ju-zhen.html" rel="alternate"></link><updated>2019-01-09T16:56:43+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-09:mei-de-ju-zhen.html</id><summary type="html">&lt;p&gt;"美"的矩阵形式&lt;/p&gt;
&lt;h1&gt;identity matrix 单位矩阵&lt;/h1&gt;
&lt;h1&gt;Unitary matrix 酉矩阵&lt;/h1&gt;
&lt;h1&gt;Normal matrix 正规矩阵&lt;/h1&gt;
&lt;h1&gt;Orthogonal matrix&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;p&gt;An orthogonal matrix is a square matrix whose columns and rows are orthogonal
unit vectors (i.e., orthonormal vectors), i.e.
&lt;/p&gt;
&lt;div class="math"&gt;$$Q^{T} Q = Q Q^{T} = I$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(I\)&lt;/span&gt; is the identity matrix.
This leads to the equivalent characterization: a matrix Q is orthogonal if
its transpose is equal to its inverse:
&lt;/p&gt;
&lt;div class="math"&gt;$$Q^{T} = Q^{−1}. $$&lt;/div&gt;
&lt;p&gt;An orthogonal matrix Q is necessarily invertible (with inverse &lt;span class="math"&gt;\(Q^{−1} = Q^{T}\)&lt;/span&gt;),
unitary (&lt;span class="math"&gt;\(Q^{−1} = Q^{∗}\)&lt;/span&gt;) and therefore normal (&lt;span class="math"&gt;\(Q^{∗}Q = QQ^{∗}\)&lt;/span&gt;) in the reals.
The determinant of any orthogonal matrix is either +1 or −1. As a linear
transformation, an orthogonal matrix preserves the dot product of vectors, and
therefore acts as an isometry of Euclidean space, such as a rotation, reflection
or rotoreflection. In other words, it is a unitary transformation.&lt;/p&gt;
&lt;p&gt;The set of n × n orthogonal matrices forms a group &lt;span class="math"&gt;\(O(n)\)&lt;/span&gt;, known as
the orthogonal group. The subgroup &lt;span class="math"&gt;\(SO(n)\)&lt;/span&gt; consisting of orthogonal
matrices with determinant +1 is called the special orthogonal
group, and each of its elements is a special orthogonal matrix. As
a linear transformation, every special orthogonal matrix acts as a
rotation.&lt;/p&gt;
&lt;h2&gt;Decompositions&lt;/h2&gt;
&lt;p&gt;A number of important matrix decompositions (Golub &amp;amp; Van Loan 1996) involve
orthogonal matrices, including especially:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;QR decomposition&lt;br /&gt;
    M = QR, Q orthogonal, R upper triangular&lt;/li&gt;
&lt;li&gt;Singular value decomposition&lt;br /&gt;
    M = UΣVT, U and V orthogonal, Σ diagonal matrix&lt;/li&gt;
&lt;li&gt;Eigendecomposition of a symmetric matrix (decomposition according to the spectral theorem)&lt;br /&gt;
    S = QΛQT, S symmetric, Q orthogonal, Λ diagonal&lt;/li&gt;
&lt;li&gt;Polar decomposition&lt;br /&gt;
    M = QS, Q orthogonal, S symmetric non-negative definite&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;diagonal matrix&lt;/h1&gt;
&lt;h2&gt;diagonal matrix with eigenvalue&lt;/h2&gt;
&lt;h1&gt;orthonormal matrix&lt;/h1&gt;
&lt;h1&gt;symmetric matrix:&lt;/h1&gt;
&lt;p&gt;A symmetric matrix is a square matrix that is equal to its transpose. Formally,
&lt;/p&gt;
&lt;div class="math"&gt;$$A^{T} = A$$&lt;/div&gt;
&lt;p&gt;
A nxn symmetric matrix A not only has a nice structure, but it also satisfies
the following (can be diagonaized, S = QΛQT, S symmetric, Q orthogonal, Λ diagonal):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A has exactly n (not necessarily distinct) eigenvalues&lt;/li&gt;
&lt;li&gt;There exists a set of n eigenvectors, one for each eigenvalue, that are mututally orthogonal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, the situation encountered with the matrix D in the example above cannot
happen with a symmetric matrix: A symmetric matrix has n eigenvalues and there
exist n linearly independent eigenvectors (because of orthogonality) even if
the eigenvalues are not distinct.&lt;/p&gt;
&lt;p&gt;Proof of eigenvalue properties of the real symmetric matrix&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;矩阵的相似变换可以把一个比较丑的矩阵变成一个比较美的矩阵，而保证这两个矩阵都是描述了同一个线性变换。至于什么样的矩阵是“美”的，什么样的是“丑”的，我们说对角阵是美的。在线性代数中，我们会看到，如果把复杂的矩阵变换成对角矩阵，作用完了之后再变换回来，这种转换很有用处，比如求解矩阵的n次幂！而学了矩阵论之后你会发现，矩阵的n次幂是工程中非常常见的运算。这里顺便说一句，将矩阵对角化在控制工程和机械振动领域具有将复杂方程解耦的妙用！总而言之，相似变换是为了简化计算！&lt;/p&gt;
&lt;p&gt;从另一个角度理解矩阵就是：矩阵主对角线上的元素表示自身和自身的关系，其他位置的元素aij表示i位置和j位置元素之间的相互关系。那么好，特征值问题其实就是选取了一组很好的基，就把矩阵 i位置和j位置元素之间的相互关系消除了。而且因为是相似变换，并没有改变矩阵本身的特性。因此矩阵对角化才如此的重要！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;similar matrix:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;have same eigenvalues&lt;/li&gt;
&lt;li&gt;matrices represent the same thing in different basis&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;projection matrix:&lt;/h1&gt;
&lt;p&gt;In linear algebra and functional analysis, a projection is a linear
transformation P from a vector space to itself such that &lt;span class="math"&gt;\(P^2 = P\)&lt;/span&gt;. That is,
whenever P is applied twice to any value, it gives the same
result as if it were applied once (idempotent). It leaves its
image unchanged.[1] Though abstract, this definition of
"projection" formalizes and generalizes the idea of graphical
projection. One can also consider the effect of a projection on
a geometrical object by examining the effect of the projection
on points in the object.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Orthogonal_matrix &amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;http://farside.ph.utexas.edu/teaching/336k/Newton/node66.html&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>Positive definite matrix</title><link href="/positive-definite-matrix.html" rel="alternate"></link><updated>2019-01-09T16:03:03+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-09:positive-definite-matrix.html</id><summary type="html">&lt;h1&gt;Definition&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;p&gt;In linear algebra, a &lt;strong&gt;symmetric&lt;/strong&gt; &lt;span class="math"&gt;\(n \times n\)&lt;/span&gt; real matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is said to be
positive definite if the scalar &lt;span class="math"&gt;\(z^{T}Mz\)&lt;/span&gt; is strictly positive for every
non-zero column vector &lt;span class="math"&gt;\(z\)&lt;/span&gt; of &lt;span class="math"&gt;\(n\)&lt;/span&gt; real numbers. Here &lt;span class="math"&gt;\(z^{\textsf {T}}\)&lt;/span&gt; denotes
the transpose of &lt;span class="math"&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;More generally, an &lt;span class="math"&gt;\(n \times n\)&lt;/span&gt; Hermitian matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is said to be positive
definite if the scalar &lt;span class="math"&gt;\(z^{*}Mz\)&lt;/span&gt; is strictly positive for every non-zero column
vector &lt;span class="math"&gt;\(z\)&lt;/span&gt; of &lt;span class="math"&gt;\(n\)&lt;/span&gt; complex numbers. Here &lt;span class="math"&gt;\(z^{*}\)&lt;/span&gt; denotes the conjugate transpose
of &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(z^{∗} M z\)&lt;/span&gt; is automatically real since &lt;span class="math"&gt;\(M\)&lt;/span&gt; is Hermitian.&lt;/p&gt;
&lt;h1&gt;与Diagonalization, eigenvalues和eigenvectorg关系&lt;/h1&gt;
&lt;h1&gt;Quadratic forms, convexity, optimization&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;p&gt;The (purely) quadratic form associated with a real &lt;span class="math"&gt;\(n\times n\)&lt;/span&gt; matrix M is the
function &lt;span class="math"&gt;\(Q : \mathbb {R}^n \to \mathbb R\)&lt;/span&gt; such that &lt;span class="math"&gt;\(Q(x)=x^{T}Mx\)&lt;/span&gt; for all
&lt;span class="math"&gt;\(x\)&lt;/span&gt;. &lt;span class="math"&gt;\(M\)&lt;/span&gt; can be assumed symmetric by replacing it with  &lt;span class="math"&gt;\(1/2(M+M^{T})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A symmetric matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is positive definite if and only if its quadratic form
is a strictly &lt;strong&gt;convex&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;More generally, any quadratic function from &lt;span class="math"&gt;\(\mathbb {R}^{n}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbb {R}\)&lt;/span&gt;
can be written as &lt;span class="math"&gt;\( x^{T}Mx+x^{T}b+c\)&lt;/span&gt; where &lt;span class="math"&gt;\(M\)&lt;/span&gt; is a symmetric &lt;span class="math"&gt;\(n\times n\)&lt;/span&gt;
matrix, &lt;span class="math"&gt;\(b\)&lt;/span&gt; is a real &lt;span class="math"&gt;\(n\)&lt;/span&gt;-vector, and &lt;span class="math"&gt;\(c\)&lt;/span&gt; a real constant. This quadratic
function is strictly convex, and hence has a unique finite global minimum, if
and only if &lt;span class="math"&gt;\(M\)&lt;/span&gt; is positive definite. For this reason, positive definite
matrices play an important role in optimization problems.&lt;/p&gt;
&lt;h1&gt;Additional information:&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://stats.stackexchange.com/questions/224005/why-are-symmetric-positive-definite-spd-matrices-so-important"&gt;StackExchange: Why are symmetric positive definite (SPD) matrices so important?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://setosa.io/ev/eigenvectors-and-eigenvalues/"&gt;Eigenvectors and Eigenvalues: Explained Visually&lt;/a&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Definiteness_of_a_matrix&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="Optimization"></category></entry><entry><title>EigenValue EigenVector</title><link href="/eigenvalue-eigenvector.html" rel="alternate"></link><updated>2019-01-06T17:22:56+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-06:eigenvalue-eigenvector.html</id><summary type="html">&lt;p&gt;我们知道，矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。&lt;/p&gt;
&lt;p&gt;特征值eigenvalue和特征向量eigenvector的一大应用是用于大量数据的降维
压缩，简化计算
相似矩阵&lt;/p&gt;
&lt;p&gt;https://zhuanlan.zhihu.com/p/31003468&lt;/p&gt;
&lt;p&gt;http://setosa.io/ev/eigenvectors-and-eigenvalues/&lt;/p&gt;
&lt;p&gt;https://www.zhihu.com/question/21874816&lt;/p&gt;
&lt;p&gt;http://mlwiki.org/index.php/Eigendecomposition&lt;/p&gt;
&lt;p&gt;https://blog.csdn.net/dongtinghong/article/details/14216139&lt;/p&gt;
&lt;p&gt;https://stats.stackexchange.com/questions/224005/why-are-symmetric-positive-definite-spd-matrices-so-important&lt;/p&gt;
&lt;p&gt;Still, it is important to know what determinants are, and their basic properties. In 18.06, we mainly use determinants as a conceptual tool to help us understand eigenvalues via the characteristic polynomial — although, again, this is not a practical computational tool for eigenvalues, which are nowadays computed by very different methods.&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Singular matrix</title><link href="/singular-matrix.html" rel="alternate"></link><updated>2019-01-05T12:26:08+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-05:singular-matrix.html</id><summary type="html">&lt;p&gt;Singular Matrix or Noninvertible Matrix&lt;/p&gt;
&lt;p&gt;A square matrix which does not have an inverse. A matrix is singular if and only if its determinant is zero.&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Normal Equation</title><link href="/normal-equation.html" rel="alternate"></link><updated>2019-01-04T18:08:16+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-04:normal-equation.html</id><summary type="html">&lt;p&gt;从MIT公开课Linear Algebra从了解到，这里做了很好的总结.
在线性方程组条件下，Normal equation解法与梯度下降法一样，梯度下降法一次迭代就可以稳定。可以证明，与正规方程解法意义相同。同时，正规方程解法具有几何意义.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/u013802188/article/details/40181601"&gt;CSDN:正规方程组(The normal equations)&lt;/a&gt;
&lt;a href="http://mlwiki.org/index.php/Normal_Equation"&gt;ML Wiki: Normal equation&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>Interpolation</title><link href="/interpolation.html" rel="alternate"></link><updated>2019-01-03T23:32:10+01:00</updated><author><name>Nekoo</name></author><id>tag:,2019-01-03:interpolation.html</id><summary type="html">&lt;p&gt;In the mathematical field of numerical analysis, interpolation is a method of
constructing new data points within the range of a discrete set of known data
points.&lt;/p&gt;
&lt;p&gt;In engineering and science, one often has a number of data points, obtained by
sampling or experimentation, which represent the values of a function for a
limited number of values of the independent variable. It is often required to
interpolate, i.e., estimate the value of that function for an intermediate
value of the independent variable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;linear interpolation.
Linear interpolation is quick and easy, but&lt;ul&gt;
&lt;li&gt;It is not very precise.&lt;/li&gt;
&lt;li&gt;The interpolant is not differentiable at the point &lt;span class="math"&gt;\(x_k\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Polynomial interpolation
Overcomes most of the problems of linear interpolation. (error, differentiable)
However, polynomial interpolation also has some disadvantages.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculating the interpolating polynomial is computationally expensive
compared to linear interpolation.&lt;/li&gt;
&lt;li&gt;polynomial interpolation may exhibit oscillatory artifacts,
  especially at the end points (see Runge's phenomenon).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spline interpolation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;smaller error than linear interpolation&lt;/li&gt;
&lt;li&gt;interpolant is smoother.&lt;/li&gt;
&lt;li&gt;the interpolant is easier to evaluate than the high-degree polynomials used
in polynomial interpolation&lt;/li&gt;
&lt;li&gt;However, the global nature of the basis functions leads to
ill-conditioning. This is completely mitigated by using splines of compact
support, such as are implemented in Boost.Math and discussed in Kress.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A simple (well behaved) differentiable interpolation is the Cubic
Hermite Spline&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Interpolation#Polynomial_interpolation"&gt;Wiki: Interpolation&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>Matrix rank</title><link href="/matrix-rank.html" rel="alternate"></link><updated>2018-12-26T13:07:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-26:matrix-rank.html</id><summary type="html">&lt;ul&gt;
&lt;li&gt;直观：「秩」是图像经过矩阵变换之后的空间维度&lt;/li&gt;
&lt;li&gt;定义：「秩」是列空间的维度&lt;/li&gt;
&lt;li&gt;物理意义：From an applied setting, rank of a matrix denotes the information content of
the matrix. The lower the rank, the lower is the "information content".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我的理解：
&lt;/p&gt;
&lt;div class="math"&gt;$$Ax = b, \quad \textrm {where} \quad A \in \mathbb {R}^{nxn}, x, b \in \mathbb {R}^{n}$$&lt;/div&gt;
&lt;p&gt;given a point (vector)x in &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt;, apply the transformation, map it into another point b.
As column of C is the combination of columns of A, this indicates point b is
within the columns space of A.
In other words, point b is a point inside the vector space defined by A's columns.&lt;/p&gt;
&lt;p&gt;Rank by definition from &lt;a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)"&gt;wiki&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;in linear algebra, the rank of a matrix A is the dimension of the vector space
generated (or spanned) by its columns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Additional if some columns inside A doesn't provide more information(constraint) to the
space. They don't define new dimentions. They are just the combinations (scalar multiply and vector add) 
of other 'essential' columns. The number of the 'essential' columns which defines the
vector space is rank.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/linear_solution.png" alt="Liner solution" title="linear solution" style="max-width:100%;max-height=100%"/&gt;&lt;/p&gt;
&lt;p&gt;b is any point in &lt;span class="math"&gt;\(\mathbb {R}^m\)&lt;/span&gt;. A defines a space with its column vectors.
Of course, if you want this system of equation has a solution, b must be in the
space defined by A. In another way, a given point x will be transformed into a
point in space defined by A. You might also want this process to be reversible,
so that you can recover x given b and system matrix A.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A defines the vector space &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt;, any point x in &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt; has a unique mapping point b which is in &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A defines the vector space &lt;span class="math"&gt;\(\mathbb {R}^m\)&lt;/span&gt;, a subspace in &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt;, multiple points in $\mathbb {R}^n will map to the same point in space constrained by A.&lt;/li&gt;
&lt;li&gt;A defines a subspace in &lt;span class="math"&gt;\(\mathbb {R}^m\)&lt;/span&gt;, point b in this subspace has a unique mapping x which is in &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt;. Point b which is not this subspace, however in &lt;span class="math"&gt;\(\mathbb{R}^m\)&lt;/span&gt;, could not find any mapping points x.&lt;/li&gt;
&lt;li&gt;A defines a subspace in &lt;span class="math"&gt;\(\mathbb {R}^m\)&lt;/span&gt;, point b in this subspace has more than one mapping x which is in &lt;span class="math"&gt;\(\mathbb {R}^n\)&lt;/span&gt;. Points b which is not this subspace, however in &lt;span class="math"&gt;\(\mathbb{R}^m\)&lt;/span&gt;, could not find any mapping points x.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://math.stackexchange.com/a/21107"&gt;math.stackexchange: Importance of matrix rank&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://www.zhihu.com/question/21605094"&gt;知乎&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>线性方程组</title><link href="/xian-xing-fang-cheng-zu.html" rel="alternate"></link><updated>2018-12-26T10:25:50+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-26:xian-xing-fang-cheng-zu.html</id><summary type="html">&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In general, a system with fewer equations than unknowns has infinitely many
solutions, but it may have no solution. Such a system is known as an
underdetermined system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In general(not always), a system with more equations than unknowns has no solution. Such a
system is also known as an overdetermined system.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In general, a system with the same number of equations and unknowns has a
single unique solution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;\begin{align}
Ax &amp;amp;= b, \\
A^{-1}Ax &amp;amp;= A^{-1}b, \\
x &amp;amp;= A^{-1}b,
\end{align}&lt;/div&gt;
&lt;p&gt;
如果这个方程有唯一解，变换可逆，A可逆. 这意味着，从b，可以还原应用了变换之前的x。
这个变换是可逆的。&lt;/p&gt;
&lt;p&gt;通常有两个形式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方程组个数大于变量数，无确定解，但是找到一个近似系统描述, 常见为拟合&lt;/li&gt;
&lt;li&gt;方程组有无数解，在满足一定条件的约束下，给定一个目标函数，使其最大或者最小, 优化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;线性 or 非线性系统？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>矩阵乘法</title><link href="/ju-zhen-cheng-fa.html" rel="alternate"></link><updated>2018-12-25T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-25:ju-zhen-cheng-fa.html</id><summary type="html">&lt;div class="math"&gt;$$C = AB, A \in \mathbb {R}^{m \times n}, b \in \mathbb {R}^{n \times l}, c \in \mathbb {R}^{m \times l}$$&lt;/div&gt;
&lt;p&gt;For example:
&lt;/p&gt;
&lt;div class="math"&gt;$$
A =
\begin{bmatrix}
a_{00} &amp;amp; a_{01} &amp;amp; a_{02}\\
a_{10} &amp;amp; a_{11} &amp;amp; a_{12}\\
a_{20} &amp;amp; a_{21} &amp;amp; a_{22}
\end{bmatrix}
B =
\begin{bmatrix}
b_{00} &amp;amp; b_{01} &amp;amp; b_{02}\\
b_{10} &amp;amp; b_{11} &amp;amp; b_{12}\\
b_{20} &amp;amp; b_{21} &amp;amp; b_{22}
\end{bmatrix}
C =
\begin{bmatrix}
c_{00} &amp;amp; c_{01} &amp;amp; c_{02}\\
c_{10} &amp;amp; c_{11} &amp;amp; c_{12}\\
c_{20} &amp;amp; c_{21} &amp;amp; c_{22}
\end{bmatrix}
$$&lt;/div&gt;
&lt;h1&gt;Componenet wise&lt;/h1&gt;
&lt;div class="math"&gt;$$C_{ij} = \sum_{k=1}^{m} a_{ik} \times b_{kj}$$&lt;/div&gt;
&lt;p&gt;
for i = 1, ..., m and j = 1, ..., l.&lt;/p&gt;
&lt;h1&gt;Row wise&lt;/h1&gt;
&lt;p&gt;Row of C is the combination of rows of B&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
c_{00} = a_{00} \times b_{00} + a_{01} \times b_{10} + a_{02} \times b_{20} \\
c_{01} = a_{00} \times b_{01} + a_{01} \times b_{11} + a_{02} \times b_{21} \\
c_{03} = a_{00} \times b_{02} + a_{01} \times b_{12} + a_{02} \times b_{22} \\
\end{cases} \rightarrow
\begin{bmatrix}
c_{00} \\ c_{01} \\ c_{02}
\end{bmatrix}^T =
\begin{bmatrix}
b_{00} \\ b_{01} \\ b_{02}
\end{bmatrix} ^T
\times  a_{00} +
\begin{bmatrix}
b_{10} \\ b_{11} \\ b_{12}
\end{bmatrix}^T
\times  a_{01} +
\begin{bmatrix}
b_{20} \\ b_{21} \\ b_{22}
\end{bmatrix}^T
\times  a_{02}
$$&lt;/div&gt;
&lt;h1&gt;Column wise&lt;/h1&gt;
&lt;p&gt;Column of C is the combination of columns of A
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
c_{00} = a_{00} \times b_{00} + a_{01} \times b_{10} + a_{02} \times b_{20} \\
c_{10} = a_{10} \times b_{00} + a_{11} \times b_{10} + a_{12} \times b_{20} \\
c_{20} = a_{20} \times b_{00} + a_{21} \times b_{10} + a_{22} \times b_{20} \\
\end{cases} \rightarrow
\begin{bmatrix}
c_{00} \\ c_{10} \\ c_{20}
\end{bmatrix} =
\begin{bmatrix}
a_{00} \\ a_{10} \\ a_{20}
\end{bmatrix}
\times  b_{00} +
\begin{bmatrix}
a_{01} \\ a_{11} \\ a_{21}
\end{bmatrix}
\times  b_{10} +
\begin{bmatrix}
a_{02} \\ a_{12} \\ a_{22}
\end{bmatrix}
\times  b_{20}
$$&lt;/div&gt;
&lt;h1&gt;Matrix wise&lt;/h1&gt;
&lt;p&gt;AB is the sum of col_of(A) * row_of(B)
&lt;/p&gt;
&lt;div class="math"&gt;$$AB = \sum_{i=0}^l {A[:i] \times B[i:]}$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(a[:i] \in \mathbb {R}^{mx1}, B[i:] \in \mathbb {R}^{1xl}\)&lt;/span&gt;&lt;/p&gt;
&lt;h1&gt;Block matrix multiplication&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_multiplication"&gt;Wiki&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>矩阵分解</title><link href="/ju-zhen-fen-jie.html" rel="alternate"></link><updated>2018-12-25T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-25:ju-zhen-fen-jie.html</id><summary type="html">&lt;h1&gt;Simplify the computation of linear system of equations.&lt;/h1&gt;
&lt;p&gt;Matrix factorization in the context of numerical linear algebra(NLA) generally serves the purpose of rephrasing through a series of easier subproblems a task that may be relatively difficult to solve in its original form.&lt;/p&gt;
&lt;p&gt;For example, given the typical linear system Ax = b for &lt;span class="math"&gt;\(A \in \mathbb {R}^{n × n}\)&lt;/span&gt;, x and b &lt;span class="math"&gt;\(\in \mathbb {R}^n\)&lt;/span&gt; , a factorization of A as LU for L a unit lower triangular matrix (thus, with ones along its main diagonal) and an upper triangular U is a mechanism for characterizing what occurs in Gaussian elimination.&lt;/p&gt;
&lt;p&gt;We replace &lt;span class="math"&gt;\(Ax = b\)&lt;/span&gt; by two (easier to solve) triangular systems:  find y so &lt;span class="math"&gt;\(Ly = b\)&lt;/span&gt; and then find x so &lt;span class="math"&gt;\(Ux = y\)&lt;/span&gt;.The point being made here is that the factorization of A as LU has no real importance in and of itself other than as a computationally convenient means for obtaining a solution to the original linear system.&lt;/p&gt;
&lt;h1&gt;Help to understand the structure of a big data matrix.&lt;/h1&gt;
&lt;p&gt;Typically, a matrix &lt;span class="math"&gt;\(A \in \mathbb {R}^{n × p}\)&lt;/span&gt; represents a data matrix containing numerical observations on n objects (subjects) over p attributes (variables), or possibly &lt;span class="math"&gt;\(B \in \mathbb {R}^{p × p}\)&lt;/span&gt; and the entries are some measure of proximity between attributes, such as the correlation between columns of A.&lt;/p&gt;
&lt;p&gt;The major purpose of a matrix factorization in this context is to obtain some form of lower-rank (and therefore simplified) approximation to A (or possibly to B) for understanding the structure of the data matrix, particularly the relationship within the objects and within the attributes, and how the objects relate to the attributes. If we can further interpret the matrix factorization geometrically and actually present the results spatially through coordinates obtained from the components of the factorization, we will be able to better communicate to others what structure may be present in the original data matrix.&lt;/p&gt;
&lt;p&gt;In any case, matrix factorizations are again directed toward the issue of simplicity, but now the actual components making up a factorization are of prime concern and not solely as a mechanism for solving another problem.&lt;/p&gt;
&lt;h1&gt;Related concepts&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)"&gt;Rank (linear algebra)秩&lt;/a&gt;&lt;blockquote&gt;
&lt;p&gt;In linear algebra, the rank of a matrix A is the dimension of the vector space generated (or spanned) by its columns.[1] This corresponds to the maximal number of linearly independent columns of A.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Numerical_stability"&gt;Numerical stability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Related articles&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://staff.ustc.edu.cn/~ynyang/group-meeting/2014/matrix-factorization/hubert.pdf"&gt;Two Purposes for Matrix Factorization: A Historical Appraisal&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://people.duke.edu/~ccc14/sta-663/LinearAlgebraMatrixDecompWithSolutions.html"&gt;Linear Algebra and Matrix Decompositions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.jiqizhixin.com/articles/0301"&gt;奇异值分解简介：从原理到基础机器学习应用&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>矩阵性质 (From Wiki)</title><link href="/ju-zhen-xing-zhi-from-wiki.html" rel="alternate"></link><updated>2018-12-25T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-25:ju-zhen-xing-zhi-from-wiki.html</id><summary type="html">&lt;h1&gt;Square matrix:&lt;/h1&gt;
&lt;p&gt;In mathematics, a square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied.&lt;/p&gt;
&lt;p&gt;Square matrices are often used to represent simple linear transformations, such as shearing or rotation. For example, if R is a square matrix representing a rotation (rotation matrix) and v is a column vector describing the position of a point in space, the product Rv yields another column vector describing the position of that point after that rotation. If v is a row vector, the same transformation can be obtained using vRT, where RT is the transpose of R.&lt;/p&gt;
&lt;h1&gt;Identity matrix:&lt;/h1&gt;
&lt;p&gt;In linear algebra, the identity matrix, or sometimes ambiguously called a unit matrix, of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere.  It is denoted by &lt;span class="math"&gt;\(I_n\)&lt;/span&gt;, or simply by I if the size is immaterial or can be trivially determined by the context.&lt;/p&gt;
&lt;h1&gt;Symmetric matrix:&lt;/h1&gt;
&lt;p&gt;In linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. Formally,&lt;/p&gt;
&lt;div class="math"&gt;$$A = A^T$$&lt;/div&gt;
&lt;h1&gt;Diagonal matrx:&lt;/h1&gt;
&lt;p&gt;In linear algebra, a diagonal matrix is a matrix in which the entries outside the main diagnoal are all zero. The term usually refers to square matrices.&lt;/p&gt;
&lt;h1&gt;Triangular matrix&lt;/h1&gt;
&lt;p&gt;In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. A square matrix is called &lt;strong&gt;lower triangular&lt;/strong&gt; if all the entries above the main diagonal are zero. Similarly, a square matrix is called &lt;strong&gt;upper triangular&lt;/strong&gt; if all the entries below the main diagonal are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a diagonal matrix.&lt;/p&gt;
&lt;p&gt;Because matrix equations with triangular matrices are easier to solve, they are very important in numerical analysis. By the LU decomposition algorithm, an invertible matrix may be written as the product of a lower triangular matrix L and an upper triangular matrix U if and only if all its leading principal minors are non-zero.&lt;/p&gt;
&lt;h1&gt;Invertible matrix&lt;/h1&gt;
&lt;p&gt;In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbf {AB} =\mathbf {BA} =\mathbf {I}$$&lt;/div&gt;
&lt;p&gt;
where In denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by &lt;span class="math"&gt;\(A^{−1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.&lt;/p&gt;
&lt;p&gt;The set of &lt;span class="math"&gt;\(n × n\)&lt;/span&gt; invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree n.&lt;/p&gt;
&lt;h1&gt;Positive-definite matrix&lt;/h1&gt;
&lt;p&gt;In linear algebra, a symmetric &lt;span class="math"&gt;\(n × n\)&lt;/span&gt; real matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; is said to be positive definite if the scalar &lt;span class="math"&gt;\(z^{\textsf {T}}Mz\)&lt;/span&gt; is strictly positive for every non-zero column vector &lt;span class="math"&gt;\(z\)&lt;/span&gt; of &lt;span class="math"&gt;\(n\)&lt;/span&gt; real numbers. Here &lt;span class="math"&gt;\(z^{\textsf {T}}\)&lt;/span&gt; denotes the transpose of &lt;span class="math"&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Orthogonal matrix&lt;/h1&gt;
&lt;p&gt;An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.
    &lt;/p&gt;
&lt;div class="math"&gt;$$Q^TQ=QQ^T = I$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(I\)&lt;/span&gt; I is the identity matrix.&lt;br /&gt;
This leads to the equivalent characterization: a matrix Q is orthogonal if its transpose is equal to its inverse:
&lt;/p&gt;
&lt;div class="math"&gt;$$Q^{\mathrm {T} }=Q^{-1}$$&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
An orthogonal matrix Q is necessarily invertible (with inverse &lt;span class="math"&gt;\(Q^−1 = Q^T\)&lt;/span&gt;), unitary (&lt;span class="math"&gt;\(Q^−1 = Q^∗\)&lt;/span&gt;) and therefore normal (&lt;span class="math"&gt;\(Q^∗Q = QQ^∗\)&lt;/span&gt;) in the reals. The determinant of any orthogonal matrix is either +1 or −1. As a linear transformation, an orthogonal matrix preserves the dot product of vectors, and therefore acts as an isometry of Euclidean space, such as a rotation, reflection or rotoreflection. In other words, it is a unitary transformation.&lt;br /&gt;
The set of n × n orthogonal matrices forms a group O(n), known as the orthogonal group. The subgroup SO(n) consisting of orthogonal matrices with determinant +1 is called the special orthogonal group, and each of its elements is a special orthogonal matrix. As a linear transformation, every special orthogonal matrix acts as a rotation. &lt;/p&gt;
&lt;h1&gt;Unitary matrix (酉矩阵)&lt;/h1&gt;
&lt;p&gt;In mathematics, a complex square matrix U is unitary if its conjugate transpose &lt;span class="math"&gt;\(U^*\)&lt;/span&gt; is also its inverse—that is, if
&lt;/p&gt;
&lt;div class="math"&gt;$$U^∗ U = U U^∗ = I$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(I\)&lt;/span&gt; is the identity matrix. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category></entry><entry><title>计算机求导的四种方式</title><link href="/ji-suan-ji-qiu-dao-de-si-chong-fang-shi.html" rel="alternate"></link><updated>2018-12-21T15:08:08+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-21:ji-suan-ji-qiu-dao-de-si-chong-fang-shi.html</id><summary type="html">&lt;p&gt;计算机求导，有四种方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人工解析微分法（manual analytical differentiation）&lt;/li&gt;
&lt;li&gt;数值微分法（numerical differentiation）&lt;/li&gt;
&lt;li&gt;符号微分法（symbolic differentiation）&lt;/li&gt;
&lt;li&gt;自动微分法（automatic differentiation）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/differentiation.jpeg" alt="Automatic differentiation in
machine learning: a survey" title="differentiation" style="max-width: 100%; max-height: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;参考:&lt;br /&gt;
&lt;a href="http://ceres-solver.org/derivatives.html"&gt;Ceres Derivatives&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://toutiao.io/posts/a5qqmj/preview"&gt;计算机求导的四种方式&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://arxiv.org/abs/1502.05767"&gt;Automatic differentiation in machine learning: a survey&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category></entry><entry><title>line search</title><link href="/line-search.html" rel="alternate"></link><updated>2018-12-19T22:23:31+00:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-19:line-search.html</id><summary type="html">&lt;p&gt;From &lt;a href="https://en.wikipedia.org/wiki/Line_search"&gt;Wiki&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In optimization, the line search strategy is one of two basic iterative 
approaches to find a local minimum &lt;span class="math"&gt;\(\mathbf {x}^{*}\)&lt;/span&gt; of an objective function
&lt;span class="math"&gt;\(\mathbf {f} :\mathbb {R} ^{n} \to \mathbb {R}\)&lt;/span&gt;. The other approach is 
&lt;a href="https://en.wikipedia.org/wiki/Trust_region"&gt;trust region&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The line search approach first finds a descent direction along which the 
objective function &lt;span class="math"&gt;\(f\)&lt;/span&gt; will be reduced and then computes a 
step size that determines how far &lt;span class="math"&gt;\(\mathbf {x}\)&lt;/span&gt; should move along that 
direction. The descent direction can be computed by various methods, such as 
gradient descent, Newton's method and Quasi-Newton method. The step size can be 
determined either exactly or inexactly.&lt;/p&gt;
&lt;p&gt;trust-region methods first choose a step size (the size of the trust region) 
and then a step direction, while line-search methods first choose a step 
direction and then a step size.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在置信域方法中，优化问题构成了一个不等式约束优化问题，此问题使用拉格朗日乘子的方式，将它转换成无约束优化问题. 具体算法案例有Levenberg-Marquardt algorithm.&lt;/p&gt;
&lt;p&gt;gradient descent, 一阶偏导, jacobian matrix&lt;br/&gt;
Newtone method, 二阶偏导, Hessian matrix&lt;br/&gt;
Guass-Newton method, 使用Jacobian matrix近似Hessian matrix &lt;span class="math"&gt;\(\mathbf {H} \approx 2\mathbf {J_r^T} \mathbf{J_r}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Gradient_descent"&gt;Wiki: gradient descent&lt;/a&gt;&lt;br/&gt;
&lt;a href="https://zhuanlan.zhihu.com/p/32709034"&gt;【最优化】一文搞懂最速下降法&lt;/a&gt;
&lt;a href="https://zhuanlan.zhihu.com/p/33413665"&gt;知乎：非线性优化&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>Gradient梯度</title><link href="/gradientti-du.html" rel="alternate"></link><updated>2018-12-19T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-19:gradientti-du.html</id><summary type="html">&lt;p&gt;The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued. &lt;/p&gt;
&lt;p&gt;In some applications it is customary to represent the gradient as a row vector or column vector of its components
in a rectangular coordinate system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;muti-variable function&lt;ul&gt;
&lt;li&gt;scalar-valued function &lt;span class="math"&gt;\(f: \mathbb{R}^n \to \mathbb{R}\)&lt;/span&gt;, gradient vector, row matrix or column matrix.&lt;br/&gt;
&lt;span class="math"&gt;\(f(\mathbf{x})\)&lt;/span&gt; where &lt;span class="math"&gt;\(\mathbf(x) = (x1, x2,...,xn)\)&lt;/span&gt;.
&lt;div class="math"&gt;\begin{align*}
\def\pdiff#1#2{\frac{\partial #1}{\partial #2}}
Df(\mathbf{a}) = \left[\pdiff{f}{x_1}(\mathbf{a}) \ \pdiff{f}{x_2}(\mathbf{a}) \ \ldots \ 
\pdiff{f}{x_n}(\mathbf{a})\right].
\end{align*}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;vector-valued function &lt;span class="math"&gt;\(\boldsymbol{f}: \mathbb{R}^n \to \mathbb{R}^m\)&lt;/span&gt;, mxn matrix. Jacobian matrix.&amp;lt;br&gt;
&lt;div class="math"&gt;\begin{gather*}
\mathbf{f}(\mathbf{x}) = (f_1(\mathbf{x}),f_2(\mathbf{x}), \cdots, f_m(\mathbf{x}))
=
\left[\begin{array}{c}
f_1(\mathbf{x})\\f_2(\mathbf{x})\\ \vdots\\ f_m(\mathbf{x})
\end{array}\right].
\end{gather*}&lt;/div&gt;
&lt;div class="math"&gt;\begin{gather*}
\def\pdiff#1#2{\frac{\partial #1}{\partial #2}}
D\mathbf{f}(\mathbf{a})=
\left[
\begin{array}{cccc}
\displaystyle\pdiff{f_1}{x_1}(\mathbf{a})&amp;amp;
\displaystyle\pdiff{f_1}{x_2}(\mathbf{a})&amp;amp;
\ldots &amp;amp;
\displaystyle\pdiff{f_1}{x_n}(\mathbf{a})\\
    \displaystyle\pdiff{f_2}{x_1}(\mathbf{a})&amp;amp;
    \displaystyle\pdiff{f_2}{x_2}(\mathbf{a})&amp;amp;
    \ldots &amp;amp;
    \displaystyle\pdiff{f_2}{x_n}(\mathbf{a})\\
        \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots\\
        \displaystyle\pdiff{f_m}{x_1}(\mathbf{a})&amp;amp;
        \displaystyle\pdiff{f_m}{x_2}(\mathbf{a})&amp;amp;
\ldots &amp;amp;
\displaystyle\pdiff{f_m}{x_n}(\mathbf{a})
\end{array}
\right].
\end{gather*}&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linear approximation to a function
The gradient of a function f from the Euclidean space &lt;span class="math"&gt;\(\mathbb{R}^n \to \mathbb{R}\)&lt;/span&gt;
at any particular point x0 in &lt;span class="math"&gt;\(\mathbb{R}^n\)&lt;/span&gt; characterizes the best linear
approximation to f at x0. The approximation is as follows:
    &lt;div class="math"&gt;$$f(x) \approx f (x0) + \nabla f(x0) \cdot (x−x0)$$&lt;/div&gt;
for x close to x0, where &lt;span class="math"&gt;\(\nabla f(x0)\)&lt;/span&gt; is the gradient of f computed at x0,
and the dot denotes the dot product on Rn.&lt;br/&gt;
This equation is equivalent to the first two terms in the
&lt;strong&gt;multivariable Taylor series expansion&lt;/strong&gt; of f at x0.&lt;br/&gt;
The approximation is valid only when the function is differentiable, and can be
used with the points which is very close to x0.&lt;br/&gt;
This could be extended to &lt;strong&gt;multivariable linear approximation&lt;/strong&gt; case.&lt;br/&gt;
It describes the &lt;strong&gt;tangent line/plane&lt;/strong&gt; over that point.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://mathinsight.org/linear_approximation_multivariable"&gt;mathinsight: linear approximation multivariable&lt;/a&gt; &lt;br/&gt;
&lt;a href="https://mathinsight.org/derivative_matrix"&gt;mathinsight: derivative matrix&lt;/a&gt; &lt;br/&gt;
&lt;a href="https://en.wikipedia.org/wiki/Gradient"&gt;Wiki: Gradient&lt;/a&gt; &lt;br/&gt;
&lt;a href="https://zhuanlan.zhihu.com/p/24913912"&gt;为什么梯度反方向是函数值局部下降最快的方向&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>旋转矩阵，变化矩阵，旋转向量，欧拉角，四元数</title><link href="/xuan-zhuan-ju-zhen-bian-hua-ju-zhen-xuan-zhuan-xiang-liang-ou-la-jiao-si-yuan-shu.html" rel="alternate"></link><updated>2018-12-19T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-19:xuan-zhuan-ju-zhen-bian-hua-ju-zhen-xuan-zhuan-xiang-liang-ou-la-jiao-si-yuan-shu.html</id><summary type="html">&lt;h2&gt;对比&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;平滑插值&lt;/li&gt;
&lt;li&gt;紧凑&lt;/li&gt;
&lt;li&gt;无奇异性&lt;/li&gt;
&lt;li&gt;内存和运算速度更优。内存上，一个四元数只占四个浮点数；在四元数相乘时可以直接在这四个数上进行加减乘的基本运算，比旋转向量转换成旋转矩阵相乘后再转换回旋转向量要高效得多（Rodrigues 变换还涉及除法和三角函数等高级运算）。这些在嵌入式平台上是不小的优势。何况，浮点数运算总是会有误差的，运算越多，误差累计越多，所以理论上四元数相乘也有精度上的优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;transforms axis–angle coordinates to versors (unit quaternions):&lt;/h2&gt;
&lt;p&gt;the following expression transforms axis–angle coordinates to versors (unit quaternions):
&lt;/p&gt;
&lt;div class="math"&gt;$$Q=\left(\cos {\tfrac {\theta }{2}},{\boldsymbol {\omega }}\sin {\tfrac {\theta }{2}}\right)$$&lt;/div&gt;
&lt;h2&gt;Recovering the axis-angle representation&lt;/h2&gt;
&lt;p&gt;Two rotation quaternions can be combined into one equivalent quaternion by the relation:
&lt;span class="math"&gt;\(\mathbf {q} '=\mathbf {q} _{2}\mathbf {q} _{1}\)&lt;/span&gt;,
in which q′ corresponds to the rotation q1 followed by the rotation q2.
(Note that quaternion multiplication is not commutative.)&lt;/p&gt;
&lt;p&gt;The expression &lt;span class="math"&gt;\(\mathbf {q} \mathbf {p} \mathbf {q} ^{-1}\)&lt;/span&gt; rotates any vector
quaternion &lt;span class="math"&gt;\(\mathbf {p}\)&lt;/span&gt; around an axis given by the vector a &lt;span class="math"&gt;\(\mathbf {a}\)&lt;/span&gt; by
the angle &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, where a &lt;span class="math"&gt;\(\mathbf {a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; depends on the
quaternion &lt;span class="math"&gt;\(\mathbf {q} =q_{r}+q_{i}\mathbf {i} +q_{j}\mathbf {j} +q_{k}\mathbf {k}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathbf {a}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; can be found from the following equations:
    &lt;/p&gt;
&lt;div class="math"&gt;$$ (a_{x},a_{y},a_{z})\ =\ {\frac {(q_{i},q_{j},q_{k})}{\sqrt {q_{i}^{2}+q_{j}^{2}+q_{k}^{2}}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ \theta \ =\ 2\operatorname {atan2} \left({\sqrt {q_{i}^{2}+q_{j}^{2}+q_{k}^{2}}},q_{r}\right)$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\mathrm {atan2}\)&lt;/span&gt; is the two-argument arctangent. Care should be taken when the quaternion approaches a scalar, since due to degeneracy the axis of an identity rotation is not well-defined&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/EliminatedAcmer/article/details/81407176"&gt;CSDN总结&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;quaternion to axis–angle coordinates&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="n"&gt;Eigen&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Vector3d&lt;/span&gt; &lt;span class="n"&gt;toAngleAxis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;Eigen&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Quaterniond&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;quaterd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Eigen&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Quaterniond&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;quaterd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;squared_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;// Atan-based log thanks to&lt;/span&gt;
    &lt;span class="c1"&gt;//&lt;/span&gt;
    &lt;span class="c1"&gt;// C. Hertzberg et al.:&lt;/span&gt;
    &lt;span class="c1"&gt;// &amp;quot;Integrating Generic Sensor Fusion Algorithms with Sound State&lt;/span&gt;
    &lt;span class="c1"&gt;// Representation through Encapsulation of Manifolds&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;// Information Fusion, 2011&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;SMALL_EPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// If quaternion is normalized and n=1, then w should be 1;&lt;/span&gt;
        &lt;span class="c1"&gt;// w=0 should never happen here!&lt;/span&gt;
        &lt;span class="n"&gt;assert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;SMALL_EPS&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

        &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;squared_w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fabs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;SMALL_EPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;M_PI&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;M_PI&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;atan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="n"&gt;Eigen&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Quaterniond&lt;/span&gt; &lt;span class="n"&gt;toQuaterniond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="n"&gt;Eigen&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Vector3d&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;v3d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;double&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;angle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v3d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;angle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;half_theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;imag_factor&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;real_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;half_theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;SMALL_EPS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;theta_po4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta_sq&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.0208333&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta_sq&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mf"&gt;0.000260417&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta_po4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kt"&gt;double&lt;/span&gt; &lt;span class="n"&gt;sin_half_theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;half_theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin_half_theta&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Eigen&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Quaterniond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;real_factor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;imag_factor&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;v3d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;imag_factor&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;v3d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;imag_factor&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;v3d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;SOPHUS_FUNC&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;SO3&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;expAndTheta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Tangent&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;omega&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;SOPHUS_ENSURE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="k"&gt;nullptr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;must not be nullptr.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;omega&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squaredNorm&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta_sq&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;half_theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;imag_factor&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;real_factor&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;theta_po4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;48.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
            &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;3840.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta_po4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;real_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;8.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta_sq&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
            &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;384.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;theta_po4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;sin_half_theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;half_theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin_half_theta&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;real_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;half_theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;SO3&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unit_quaternion_nonconst&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
        &lt;span class="n"&gt;QuaternionMember&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;real_factor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;omega&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;omega&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;imag_factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;omega&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
    &lt;span class="n"&gt;SOPHUS_ENSURE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;squaredNorm&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;
            &lt;span class="n"&gt;Sophus&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;SO3::exp failed! omega: %, real: %, img: %&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;omega&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;real_factor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;imag_factor&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;SOPHUS_FUNC&lt;/span&gt; &lt;span class="n"&gt;TangentAndTheta&lt;/span&gt; &lt;span class="n"&gt;logAndTheta&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;const&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;TangentAndTheta&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;atan&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;squared_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;squaredNorm&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;squared_n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

    &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="c1"&gt;// Atan-based log thanks to&lt;/span&gt;
    &lt;span class="c1"&gt;//&lt;/span&gt;
    &lt;span class="c1"&gt;// C. Hertzberg et al.:&lt;/span&gt;
    &lt;span class="c1"&gt;// &amp;quot;Integrating Generic Sensor Fusion Algorithms with Sound State&lt;/span&gt;
    &lt;span class="c1"&gt;// Representation through Encapsulation of Manifolds&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;// Information Fusion, 2011&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// If quaternion is normalized and n=0, then w should be 1;&lt;/span&gt;
        &lt;span class="c1"&gt;// w=0 should never happen here!&lt;/span&gt;
        &lt;span class="n"&gt;SOPHUS_ENSURE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                &lt;span class="s"&gt;&amp;quot;Quaternion (%) should be normalized!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;coeffs&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;Scalar&lt;/span&gt; &lt;span class="n"&gt;squared_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
            &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;squared_n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;squared_w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Constants&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;::&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Scalar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;atan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tangent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;two_atan_nbyw_by_n&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;unit_quaternion&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;J&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>Bayes and Kalman Filter</title><link href="/bayes-and-kalman-filter.html" rel="alternate"></link><updated>2018-12-18T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-18:bayes-and-kalman-filter.html</id><summary type="html">&lt;p&gt;SLAM位姿与地图估计问题有两种方式，一种是滤波，一种是优化.&lt;br/&gt;
暂时搜集到的一些信息，没有自己整理&lt;/p&gt;
&lt;p&gt;Bayes Filter推导(&lt;span style="color:red"&gt;基于Bayes公式与Markov assumption&lt;/span&gt;)&lt;br/&gt;
&lt;img src="images/bayes-1.png" alt="Bayes" title="bayes" style="width:50%"/&gt;&lt;/p&gt;
&lt;p&gt;第一步用到的Bayes公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(x|y,z) = \frac{P(y|x,z)\cdot p(x|z)}{p(y|z)}$$&lt;/div&gt;
&lt;p&gt;
由此得出，贝叶斯滤波器分为两步：&lt;br/&gt;
1. 状态预测，基于状态转移模型：
&lt;/p&gt;
&lt;div class="math"&gt;$$\overline {bel} ({x_t}) = \int {p({x_t}|{u_t},{x_{t - 1}})} \;bel({x_{t - 1}})\;d{x_{t - 1}}$$&lt;/div&gt;
&lt;p&gt;
2. 状态更新，基于新的观测
&lt;/p&gt;
&lt;div class="math"&gt;$$bel({x_t}) = \;\eta \,p({z_t}|{x_t})\,\overline {bel} ({x_t})$$&lt;/div&gt;
&lt;p&gt;在此式中&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;未进一步说明，概率机器人一书中，被称之为&lt;strong&gt;归一化因子&lt;/strong&gt;，具体表现形式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{\eta} = \sum{p(z_t|x_t)\overline{bel}(x_t)dx_t}$$&lt;/div&gt;
&lt;p&gt;
因为最后&lt;span class="math"&gt;\(bel(x_t)\)&lt;/span&gt;是一个概率，其所有&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;可能性概率的总和必须为１.&lt;/p&gt;
&lt;p&gt;伪代码流程如下: &lt;br/&gt;
&lt;img src="images/bayes-2.png" alt="Bayes" title="bayes" style="width:50%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="color:red"&gt;
同时，我们注意，我们的目的是计算&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;的后验概率，如果&lt;span class="math"&gt;\(bel(x_t)\)&lt;/span&gt;是任意分布，
我们需要在&lt;span class="math"&gt;\(x_t\)&lt;/span&gt;的所有可能取值点上，计算该取值的概率，这在计算上是难于实现的。
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;相关资料的搜集:&lt;br/&gt;
&lt;a href="/pdfs/slam02-bayes-filter-short.pdf"&gt;Bayes Filter by Cyrill Stachniss&lt;/a&gt;
&lt;br/&gt;
&lt;a href="http://ais.informatik.uni-freiburg.de/teaching/ss10/robotics/slides/10-kalman-filter.pdf"&gt;Introduction to Mobile Robotics - Bayes Filter – Kalman Filter&lt;/a&gt;
&lt;br/&gt;
&lt;a href="https://www.cnblogs.com/gaoxiang12/p/5560360.html"&gt;SLAM中的EKF，UKF，PF原理简介&lt;/a&gt;
&lt;br/&gt;
&lt;a href="https://blog.csdn.net/qq_30159351/article/details/53395515"&gt;SLAM笔记三——贝叶斯滤波器&lt;/a&gt;
&lt;br/&gt;
&lt;a href="https://blog.csdn.net/qinruiyan/article/details/50793114"&gt;SLAM学习笔记2：Kalman Filter(卡尔曼滤波) 与Least Square(最小二乘法) 的比较&lt;/a&gt;
&lt;br/&gt;
&lt;a href="https://www.cnblogs.com/ycwang16/p/5995702.html"&gt;细说贝叶斯滤波：Bayes filters&lt;/a&gt;
&lt;br/&gt;
&lt;a href="http://www.cnblogs.com/ycwang16/p/5999034.html"&gt;细说Kalman滤波：The Kalman Filter&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>非线性优化</title><link href="/fei-xian-xing-you-hua.html" rel="alternate"></link><updated>2018-12-18T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-18:fei-xian-xing-you-hua.html</id><summary type="html">&lt;p&gt;From &lt;a href="https://en.wikipedia.org/wiki/Nonlinear_programming"&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In mathematics, nonlinear programming is the process of solving an optimization
problem where some of the constraints or the objective function are nonlinear.
An optimization problem is one of calculation of the extrema
(maxima, minima or stationary points) of an objective function over a set of
unknown real variables and conditional to the satisfaction of a system of
equalities and inequalities, collectively termed constraints.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="http://blog.sina.com.cn/s/blog_7445c2940102x3x4.html"&gt;到底什么是非线性优化&lt;/a&gt;&lt;/p&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>MLE &amp; MAP</title><link href="/mle-map.html" rel="alternate"></link><updated>2018-12-18T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-18:mle-map.html</id><summary type="html">&lt;p&gt;MLE：最大似然概率 给定测量结果，系统模型，寻求系统模型参数让结果跟观测结果最贴近。&lt;/p&gt;
&lt;p&gt;似然函数（也称作似然），是一个关于统计模型参数的函数。也就是这个函数中自变量是统计模型的参数。&lt;/p&gt;
&lt;p&gt;对于观测结果 x ，在参数集合 θ 上的似然，就是在给定这些参数值的基础上，观察到的结果的概率 P(x|θ) 。
也就是说，似然是关于参数的函数，在参数给定的条件下，对于观察到的 x 的值的条件分布。&lt;/p&gt;
&lt;p&gt;MAP:最大后验概率, 考虑到先验概率的存在 prior probability.
Bayes rule
&lt;/p&gt;
&lt;div class="math"&gt;$$P(\theta|x) = \frac{P(x|\theta) * p(\theta)}{p(x)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(x|y,z) = \frac{P(y|x,z)\cdot p(x|z)}{p(y|z)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\hat{\theta} = \mathop{arg\max}_{\theta} P(\theta|x) \propto P(x|\theta) * p(\theta)$$&lt;/div&gt;
&lt;p&gt;
分母可以去掉，与求参数无关&lt;/p&gt;
&lt;p&gt;已知观测结果，寻找参数，使得模型输出最符合观测。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zh.wikipedia.org/wiki/%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"&gt;全概率公式&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/sylvanas2012/p/5058065.html"&gt;最大似然估计 （MLE） 最大后验概率（MAP）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/25768606"&gt;概率与似然&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.csdn.net/u011508640/article/details/72815981"&gt;详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>数值分析</title><link href="/shu-zhi-fen-xi.html" rel="alternate"></link><updated>2018-12-18T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-18:shu-zhi-fen-xi.html</id><summary type="html">&lt;p&gt;From
&lt;a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90"&gt;Wiki&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;数值分析（英语：numerical analysis），是指在数学分析（区别于离散数学）问题中，对使用数值近似（相对于一般化的符号运算）算法的研究。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;direct method&lt;/p&gt;
&lt;p&gt;iterative method&lt;/p&gt;
&lt;p&gt;Numerical analysis
解析解/闭合解
数值解&lt;/p&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>Variance, gradient, Jacobian and Hessian matrix 记忆</title><link href="/variance-gradient-jacobian-and-hessian-matrix-ji-yi.html" rel="alternate"></link><updated>2018-12-18T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-18:variance-gradient-jacobian-and-hessian-matrix-ji-yi.html</id><summary type="html">&lt;p&gt;Jacobian矩阵　用于一阶梯度下降法&lt;br/&gt;
Hessian矩阵，用于二阶牛顿法&lt;br/&gt;
高斯牛顿法，使用J&lt;sup&gt;T&lt;/sup&gt;J作为牛顿法中二阶Hessian矩阵的近似，　从而省略了计算Ｈ的过程&lt;br/&gt;
&lt;a href="http://jacoxu.com/jacobian%E7%9F%A9%E9%98%B5%E5%92%8Chessian%E7%9F%A9%E9%98%B5/"&gt;下面这个文章介绍了Jacobian与Hessian矩阵的关系用途&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hessian是对称矩阵&lt;br/&gt;
&lt;a href="https://blog.csdn.net/dsbatigol/article/details/12558891"&gt;jacobian, Hassian and gradient关系&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;covariance,协方差：&lt;br/&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$cov(X,Y) = E((X-\mu)(Y-\nu)) = E(X \cdot Y) - \mu\nu$$&lt;/div&gt;
&lt;p&gt;variance, 方差：　二阶矩&lt;br/&gt;
 一个随机变量的方差描述的是它的离散程度，也就是该变量离其期望值的距离, &lt;span class="math"&gt;\(\mu=E[X]\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$Var(X) = cov(X, X)$$&lt;/div&gt;
&lt;div class="math"&gt;$$Var(X) = E[(X-\mu)^2]$$&lt;/div&gt;
&lt;p&gt;期望：　一阶矩&lt;br/&gt;
&lt;a href="https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE"&gt;方差Wiki&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry><entry><title>线性规划</title><link href="/xian-xing-gui-hua.html" rel="alternate"></link><updated>2018-12-18T00:00:00+01:00</updated><author><name>Nekoo</name></author><id>tag:,2018-12-18:xian-xing-gui-hua.html</id><summary type="html">&lt;p&gt;线性优化，　线性规划，　linear programming&lt;/p&gt;</summary><category term="Math"></category><category term="SLAM"></category></entry></feed>